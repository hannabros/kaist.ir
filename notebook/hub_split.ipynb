{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KORQUAD_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/korquad'\n",
    "BOOKQA_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/bookqa'\n",
    "NEWSQA_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/newsqa'\n",
    "\n",
    "KORQUAD_TRAIN = 'KorQuAD_v1.0_train.json'\n",
    "KORQUAD_TEST = 'KorQuAD_v1.0_dev.json'\n",
    "BOOKQA_TRAIN = 'bookqa_train.json'\n",
    "BOOKQA_TEST = 'bookqa_valid.json'\n",
    "NEWS_ALL = 'ko_nia_normal_squad_all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(KORQUAD_PATH, KORQUAD_TRAIN), 'r') as f:\n",
    "    korquad_train = json.load(f)\n",
    "\n",
    "with open(os.path.join(NEWSQA_PATH, NEWS_ALL), 'r') as f:\n",
    "    news_all = json.load(f)\n",
    "\n",
    "with open(os.path.join(BOOKQA_PATH, BOOKQA_TRAIN), 'r') as f:\n",
    "    bookqa_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([5, 7, 4, 3, 2, 6, 9, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "news_dict = defaultdict(list)\n",
    "for data in news_all['data']:\n",
    "    source = data['source']\n",
    "    news_dict[source].append(data)\n",
    "print(news_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [10, 20, 30, 40, 50]\n",
    "for seed in seeds:\n",
    "    random.seed(seed)\n",
    "    news_train = defaultdict(list)\n",
    "    news_test = defaultdict(list)\n",
    "    for k, v in news_dict.items():\n",
    "        test_idx = random.choices(list(range(len(news_dict[k]))), k=len(news_dict[k])//10)\n",
    "        for ix, paragraph in enumerate(v):\n",
    "            if ix in test_idx:\n",
    "                news_test[k].append(paragraph)\n",
    "            else:\n",
    "                news_train[k].append(paragraph)\n",
    "\n",
    "    news_train_out = {\"data\": list(chain.from_iterable(news_train.values()))}\n",
    "    news_test_out = {\"data\": list(chain.from_iterable(news_test.values()))}\n",
    "\n",
    "    with open(f\"../data/newsqa/news_train_all_{seed}.json\", \"w\") as f:\n",
    "        json.dump(news_train_out, f)\n",
    "    with open(f\"../data/newsqa/news_test_all_{seed}.json\", \"w\") as f:\n",
    "        json.dump(news_test_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42802"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_out = {\"data\": news_train[1]}\n",
    "tmp_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp_out.json\" , encoding= \"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(tmp_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6618"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/ubuntu/workspace/kaist.ir/qa/data/newsqa/tmp_out.json\", 'r') as f:\n",
    "    test = json.load(f)\n",
    "len(test['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_train_tmp = {\"data\": korquad_train['data'][:20]}\n",
    "korquad_test_tmp = {\"data\": korquad_train['data'][20:30]}\n",
    "hub_train_tmp = {\"data\": news_train[1][:20]}\n",
    "hub_test_tmp = {\"data\": news_train[1][20:30]}\n",
    "\n",
    "with open(\"/home/ubuntu/workspace/kaist.ir/qa/data/tmp/korquad_train_tmp.json\" , encoding= \"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(korquad_train_tmp, f)\n",
    "\n",
    "with open(\"/home/ubuntu/workspace/kaist.ir/qa/data/tmp/korquad_test_tmp.json\" , encoding= \"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(korquad_test_tmp, f)\n",
    "\n",
    "with open(\"/home/ubuntu/workspace/kaist.ir/qa/data/tmp/hub_train_tmp.json\" , encoding= \"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(hub_train_tmp, f)\n",
    "\n",
    "with open(\"/home/ubuntu/workspace/kaist.ir/qa/data/tmp/hub_test_tmp.json\" , encoding= \"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(hub_test_tmp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets.py Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import inspect\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for the Squad dataset.\n",
    "    For examples without an answer, the start and end position are -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \", question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if len(self.orig_answer_text) != 0:\n",
    "            s += \", orig_answer_text: %s\" % (self.orig_answer_text)\n",
    "        if self.start_position != -1:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.end_position != -1:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        if self.is_impossible:\n",
    "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "        return s\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None,\n",
    "                 pooled_feature=None,\n",
    "                 cluster_idx=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "        self.pooled_feature = pooled_feature\n",
    "        self.cluster_idx = cluster_idx\n",
    "\n",
    "def _whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def read_examples(input_file, is_training):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in tqdm(input_data):\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            #pdb.set_trace()\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                is_impossible = False\n",
    "                if is_training:\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                        answer_length = len(orig_answer_text)\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                        # Only add answers where the text can be exactly recovered from the\n",
    "                        # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                        # stuff so we will just skip the example.\n",
    "                        #\n",
    "                        # Note that this means for training mode, every example is NOT\n",
    "                        # guaranteed to be preserved.\n",
    "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                        cleaned_answer_text = \" \".join(\n",
    "                            _whitespace_tokenize(orig_answer_text))\n",
    "                        if actual_text.find(cleaned_answer_text) == -1:\n",
    "                            continue\n",
    "                    else:\n",
    "                        start_position = -1\n",
    "                        end_position = -1\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=is_impossible)\n",
    "                examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 485.75it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = read_examples('/home/ubuntu/workspace/kaist.ir/qa/data/korquad/KorQuAD_v1.0_dev.json', is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[2].end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 620.37it/s]\n"
     ]
    }
   ],
   "source": [
    "is_training=True\n",
    "input_data = korquad_train['data'][1:3]\n",
    "examples = []\n",
    "for entry in tqdm(input_data):\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "        paragraph_text = paragraph[\"context\"].replace('\\u200b', '')\n",
    "        doc_tokens = []\n",
    "        prev_is_whitespace = True\n",
    "        for c in paragraph_text:\n",
    "            if is_whitespace(c):\n",
    "                prev_is_whitespace = True\n",
    "            else:\n",
    "                if prev_is_whitespace:\n",
    "                    doc_tokens.append(c)\n",
    "                else:\n",
    "                    doc_tokens[-1] += c\n",
    "                prev_is_whitespace = False\n",
    "        #pdb.set_trace()\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            qas_id = qa[\"id\"]\n",
    "            question_text = qa[\"question\"]\n",
    "            answer = qa['answers'][0]\n",
    "            is_impossible = qa['is_impossible'] if 'is_impossible' in qa.keys() else False\n",
    "            \n",
    "            if not is_impossible:\n",
    "                orig_answer_text = answer['text']\n",
    "                start_position = answer['answer_start']\n",
    "                end_position = start_position + len(orig_answer_text)\n",
    "            else:\n",
    "                start_position = -1\n",
    "                end_position = -1\n",
    "                orig_answer_text = \"\"\n",
    "        \n",
    "\n",
    "            example = SquadExample(\n",
    "                qas_id=qas_id,\n",
    "                question_text=question_text,\n",
    "                doc_tokens=doc_tokens,\n",
    "                orig_answer_text=orig_answer_text,\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=is_impossible)\n",
    "            examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qas_id: 6561639-0-0, question_text: 모든 구강기관으로 여성의 성기를 애무하는 것을 뭐라고 하는가?, doc_tokens: [커닐링구스(커닐링거스, 쿤닐링구스, 영어: Cunnilingus)는 입술, 혀, 입 등의 모든 구강기관으로 여성의 성기를 애무하는 것을 말하며 구강성교(오럴섹스, 영어: Oral sex)라고도 한다. 보통 남성이 행하며 동성의 여성이 행하는 경우도 있다. 혀를 질에 넣거나 여성의 클리토리스, 외음부나 그 주변을 핥거나 빨아서 애무한다. 받는 사람이 다양한 성감을 느끼는 원인이 되며 특히 클리토리스의 감각이 매우 중요하다. 타액과 수성윤활제가 자주 사용되고 이것들은 부드럽게 매끄러운 자극을 가능하게 한다. 파트너의 반응에 귀를 기울이면서 손가락과 같은 다른 애무와 함께 몸 전체에 다양한 자극과 결합하여 양측이 폭넓은 즐거움을 나눌 수 있게 한다. 전희로서 하는 경우가 많지만, 오르가즘에의 도달 여부에 관계없이 커닐링구스 자체가 성행위이다.], orig_answer_text: 커닐링구스, start_position: 0, end_position: 5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qas': [{'answers': [{'text': '커닐링구스', 'answer_start': 0}],\n",
       "    'id': '6561639-0-0',\n",
       "    'question': '모든 구강기관으로 여성의 성기를 애무하는 것을 뭐라고 하는가?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561639-0-1',\n",
       "    'question': '구강성교를 할 때 중요한 감각은 무엇인가?'},\n",
       "   {'answers': [{'text': '커닐링구스', 'answer_start': 0}],\n",
       "    'id': '6561642-0-0',\n",
       "    'question': '구강기관으로 여성의 성기를 애무하는 행위를 뜻하는 용어는?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561642-0-1',\n",
       "    'question': '구강성교시 특히 어느 부위의 감각이 중요한가?'},\n",
       "   {'answers': [{'text': '구강성교', 'answer_start': 80}],\n",
       "    'id': '6561653-0-0',\n",
       "    'question': '여성의 성기를 애무하는 것을 말하는 커닐링구스를 다른 말로 무엇이라고 합니까?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561653-0-1',\n",
       "    'question': '커닐링구스는 받는 사람이 다양한 성감을 느끼는 원인이 되며 무엇의 감각이 특히 중요합니까?'}],\n",
       "  'context': '커닐링구스(커닐링거스, 쿤닐링구스, 영어: Cunnilingus)는 입술, 혀, 입 등의 모든 구강기관으로 여성의 성기를 애무하는 것을 말하며 구강성교(오럴섹스, 영어: Oral sex)라고도 한다. 보통 남성이 행하며 동성의 여성이 행하는 경우도 있다. 혀를 질에 넣거나 여성의 클리토리스, 외음부나 그 주변을 핥거나 빨아서 애무한다. 받는 사람이 다양한 성감을 느끼는 원인이 되며 특히 클리토리스의 감각이 매우 중요하다. 타액과 수성윤활제가 자주 사용되고 이것들은 부드럽게 매끄러운 자극을 가능하게 한다. 파트너의 반응에 귀를 기울이면서 손가락과 같은 다른 애무와 함께 몸 전체에 다양한 자극과 결합하여 양측이 폭넓은 즐거움을 나눌 수 있게 한다. 전희로서 하는 경우가 많지만, 오르가즘에의 도달 여부에 관계없이 커닐링구스 자체가 성행위이다.'},\n",
       " {'qas': [{'answers': [{'text': '쉐어 하이트', 'answer_start': 61}],\n",
       "    'id': '6561639-1-0',\n",
       "    'question': '오르가슴은 클리토리스에 직접적인 자극을 주는 커닐링구스를 통해 쉽게 도달할 수 있다고 여성의 성 보고서를 쓴 사람은 누구인가?'},\n",
       "   {'answers': [{'text': '개인 위생', 'answer_start': 357}],\n",
       "    'id': '6561639-1-1',\n",
       "    'question': '여성이 커닐링구스를 하기 전에 중요하게 생각하는 것은 무엇인가?'},\n",
       "   {'answers': [{'text': '80%', 'answer_start': 17}],\n",
       "    'id': '6561642-1-0',\n",
       "    'question': '일반적 통계에 따르면 오르가슴을 얻기 위해 직접적인 음핵 자극을 필요로 하는 여성은 몇 퍼센트인가?'},\n",
       "   {'answers': [{'text': '쉐어 하이트', 'answer_start': 61}],\n",
       "    'id': '6561642-1-1',\n",
       "    'question': '대부분의 여성이 커닐링구스를 통해 쉽게 오르가슴에 도달할 수 있다는 성 보고서를 작성한 학자는 누구인가?'},\n",
       "   {'answers': [{'text': '음핵 자극', 'answer_start': 39}],\n",
       "    'id': '6561653-1-0',\n",
       "    'question': '일반적인 통계로 볼 때, 여성의 80%가 오르가슴을 얻으려고 직접적인 어떤 자극을 필요로 합니까?'},\n",
       "   {'answers': [{'text': '음순', 'answer_start': 208}],\n",
       "    'id': '6561653-1-1',\n",
       "    'question': '커닐링구스를 받는 여성은 파트너의 혀가 클리토리스를 잘 자극할 수 있도록 손가락을 사용하면 직접 무엇을 분리할 수 있습니까?'}],\n",
       "  'context': '일반적인 통계에 따르면 여성의 80%가 오르가슴을 얻기 위해 직접적인 음핵 자극을 필요로 한다. 성교육 학자 쉐어 하이트의 여성의 성 보고서에 따르면 대부분의 여성의 경우 오르가슴은 클리토리스에 직접적인 자극을 주는 커닐링구스를 통해 쉽게 도달할 수 있다고 보고하고 있다. 커닐링구스를 받는 여성은 파트너의 혀가 클리토리스를 더 잘 자극할 수 있도록 손가락을 사용, 직접 음순을 분리 할 수 \\u200b\\u200b있다. 또 다리를 넓게 벌리면 파트너가 클리토리스에 구두로 쉽게 도달 할 수 있도록 외음부를 드러낼 수 있다. 파트너는 음순과 전체 생식기 영역에 대해 보다 부드럽고 집중적이지 않은 자극으로 시작하는 것이 좋다. 여성은 커닐링구스를 하기 전에 개인 위생을 중요하게 생각한다. 일부 여성들은 음모를 제거하거나 다듬어 커닐링구스 경험을 향상시키고 있다.'},\n",
       " {'qas': [{'answers': [{'text': '배꼽', 'answer_start': 88}],\n",
       "    'id': '6561639-2-0',\n",
       "    'question': '성경에서 커닐링구스에 대해 우회적으로 번역한 단어는 무엇인가?'},\n",
       "   {'answers': [{'text': '포도주', 'answer_start': 116}],\n",
       "    'id': '6561639-2-1',\n",
       "    'question': '육육적인 그릇에서 마시고 싶은 남자의 욕망을 암시할 때 여성의 웅덩이에 가득 채워지고 있던 것은 무엇인가?'},\n",
       "   {'answers': [{'text': '배꼽', 'answer_start': 88}],\n",
       "    'id': '6561642-2-0',\n",
       "    'question': '아가서 중 커닐링구스에 대한 우회적 언급으로 생각할 수 있는 구절의 핵심 단어는 주로 무엇으로 번역되는가?'},\n",
       "   {'answers': [{'text': '칵테일', 'answer_start': 261}],\n",
       "    'id': '6561642-2-1',\n",
       "    'question': '아가서 7장 3절에 대한 해석 중 음부를 동그란 컵에 비유한 번역은 컵에 무엇이 끊어진 적이 없다고 하였는가?'},\n",
       "   {'answers': [{'text': '배꼽', 'answer_start': 88}],\n",
       "    'id': '6561653-2-0',\n",
       "    'question': '성경의 아가서 7:3의 핵심 단어를 많은 번역자들이 무엇으로 번역했습니까?'},\n",
       "   {'answers': [{'text': '영적인 의미', 'answer_start': 459}],\n",
       "    'id': '6561653-2-1',\n",
       "    'question': '기독교와 유대교의 전통에서 아가에 그려진 신랑과 신부의 에로틱한 친밀감에 어떤 의미를 부여하고 있습니까?'}],\n",
       "  'context': '성경의 아가서 7 : 3 (제임스 왕 역에서 7 : 2)는 커닐링구스에 대한 우회적 언급이 포함되어 있다고도 생각할 수 있지만, 많은 번역자는 핵심 단어를 \"배꼽\"로 번역하고 있다. 일부는 \"여성의 웅덩이에 포도주가 가득 채워지고 있다는 묘사는 육욕적인 그릇에서 마시고 싶은 남자의 욕망을 암시하고 있다. 따라서 이것은 교정의 미묘하고 깊은 맛의 귀띔인지도 모른다\"라는 해석을 보여주고 있다. 다른 번역에서는 다음과 같이도 읽을 수있는 - \"너의 음부는 동그란 컵, 칵테일이 끊어진 적이 없다.\"(아가 7 : 2 שררך אגן הסהר אל יחסר המזג) 샌들에서 출발 \"vulva\"(히브리어 shor-- 아람어 \\'비밀 장소\\'라는 단어에서 파생 된), 배, 가슴으로 올라 간다고 하는 문맥이 단어의 의미를 거의 결정 짓는 것이다. 기독교와 유대교의 여러 전통에서 \"아가\"에 그려진 신랑과 신부의 에로틱한 친밀감에 영적인 의미를 부여하고 있다.'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korquad_train['data'][1]['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '또한, 정책 및 사업의 본격 추진을 위한 재원의 확보 방안을 마련해야 한다. 특별법의 하위 법령 또는 관련 법령의 제·개정을 통해 불이행 행위자에 강제 이행 유도를 위한 벌금 또는 ‘오염 원인자 부담’의 원칙에 따른 ‘해양 대기오염 부담금(안)’을 부과할 수 있다. 부담금 징수로 인한 재정 수익분을 항만의 대기환경 개선을 위한 특별회계에서 수용하도록 하고, 예산의 배분에 있어서도 사업의 시급성, 파급 효과 등을 고려하여 사업의 우선순위에 차등을 두어 지원하는 ‘선택과 집중’의 전략적 방법론을 적용해야 한다. 이와 더불어 유관 부처 및 기관 등 주요 정책 행위자들의 업무 역할과 기능, 나아가 의무 및 권한의 범위 등에 대한 명확한 법적 근거 역시 시급하게 마련되어야 한다. 각 부처 및 기관별로 사업을 추진하고 개별성과를 기계적으로 취합하는 현재의 방식보다는, 통합 관리 차원에서 유관 부처·기관 간 공동 연구개발 및 조사, 공공 데이터 차원의 정보 연계·공유 및 활용 등을 통해 정책 및 사업의 효율성을 높이고, 공동의 사회 문제 해결을 위한 시너지의 도출이 필요하다.',\n",
       " 'qas': [{'question': '예산을 분배할 때 사업의 우선 순위를 정해서 차등 지원하는 방법을 뭐라고 하지',\n",
       "   'id': 1063333,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '사업의 우선 순위를 정해서 예산을 차등적으로 나눠주는 방법을 뭐라고 해',\n",
       "   'id': 1063334,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '사업의 우선 순위로 예산을 차등 분배하는 방법을 뭐라고 불러',\n",
       "   'id': 1063335,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '뭐가 사업의 크기를 정해서 예산을 차등 지원한다고 하니',\n",
       "   'id': 1063336,\n",
       "   'is_impossible': True,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookqa_train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from customBertModel import CustomizedBertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing CustomizedBertForQuestionAnswering: ['cls.predictions.decoder.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomizedBertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = CustomizedBertForQuestionAnswering.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32312/1969793652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mstart_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     end_positions=end_positions)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ir/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/kaist.ir/qa/customBertModel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_type, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, beta, sigma)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignored_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mstart_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mend_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ir/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ir/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ir/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "context = korquad_train['data'][0]['paragraphs'][0]['context']\n",
    "question = korquad_train['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "answer = korquad_train['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "start_position = korquad_train['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "end_position = start_position + len(answer)\n",
    "\n",
    "encoding = tokenizer(\n",
    "        context,\n",
    "        question,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True)\n",
    "\n",
    "start_position = encoding.char_to_token(start_position)\n",
    "end_position = encoding.char_to_token(end_position)\n",
    "\n",
    "shift = 1\n",
    "if start_position is None:\n",
    "        start_position = 512\n",
    "while end_position is None:\n",
    "        end_position = encoding.char_to_token(end_position - shift)\n",
    "        shift += 1\n",
    "\n",
    "encoding.update({'start_positions': start_position, 'end_positions': end_position})\n",
    "encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "start_positions = encoding['start_positions']\n",
    "end_positions = encoding['end_positions']\n",
    "\n",
    "outputs = model(input_ids=input_ids.reshape((1, len(input_ids))),\n",
    "                    attention_mask=attention_mask.reshape((1, len(attention_mask))),\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomizedBertForQuestionAnswering(\n",
      "  (bert): CustomizedBertModel(\n",
      "    (embeddings): CustomizedBertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ece5f9b3f3aceda458e8d77feac6fd1c43e8580863a594c877bbe067b1219d3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('ir': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
