{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book\n",
    "- is_impossible = True 인 경우 제외 필요. 대답을 할 수 없는 질문임.\n",
    "\n",
    "### News\n",
    "- source\n",
    "\n",
    "| 기입형태 | 해당 본문 카테고리 |\n",
    "|:--------:|:------------------:|\n",
    "| 1        | 정치               |\n",
    "| 2        | 경제               |\n",
    "| 3        | 사회               |\n",
    "| 4        | 생활               |\n",
    "| 5        | IT/과학            |\n",
    "| 6        | 연예               |\n",
    "| 7        | 스포츠             |\n",
    "| 8        | 문화               |\n",
    "| 9        | 미용/건강          |\n",
    "\n",
    "- classtype\n",
    "\n",
    "|  기입형태  | 해당 본문 카테고리 |\n",
    "|:----------:|:------------------:|\n",
    "| work_where | 어디서             |\n",
    "| work_who   | 누가               |\n",
    "| work_what  | 무엇을             |\n",
    "| work_how   | 어떻게             |\n",
    "| work_why   | 왜                 |\n",
    "| work_when  | 언제               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KORQUAD_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/korquad'\n",
    "BOOKQA_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/bookqa'\n",
    "NEWSQA_PATH = '/home/ubuntu/workspace/kaist.ir/qa/data/newsqa'\n",
    "\n",
    "KORQUAD_TRAIN = 'KorQuAD_v1.0_train.json'\n",
    "KORQUAD_TEST = 'KorQuAD_v1.0_dev.json'\n",
    "BOOKQA_TRAIN = 'bookqa_train.json'\n",
    "BOOKQA_TEST = 'bookqa_valid.json'\n",
    "NEWS_ALL = 'ko_nia_normal_squad_all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(KORQUAD_PATH, KORQUAD_TRAIN)) as f:\n",
    "    korquad_train = json.load(f)\n",
    "\n",
    "with open(os.path.join(KORQUAD_PATH, KORQUAD_TEST)) as f:\n",
    "    korquad_test = json.load(f)\n",
    "\n",
    "with open(os.path.join(BOOKQA_PATH, BOOKQA_TRAIN)) as f:\n",
    "    bookqa_train = json.load(f)\n",
    "\n",
    "with open(os.path.join(BOOKQA_PATH, BOOKQA_TEST)) as f:\n",
    "    bookqa_test = json.load(f)\n",
    "\n",
    "with open(os.path.join(NEWSQA_PATH, NEWS_ALL)) as f:\n",
    "    news_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}],\n",
       "   'id': '6566495-0-0',\n",
       "   'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'},\n",
       "  {'answers': [{'text': '1악장', 'answer_start': 421}],\n",
       "   'id': '6566495-0-1',\n",
       "   'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'},\n",
       "  {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}],\n",
       "   'id': '6566495-0-2',\n",
       "   'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'},\n",
       "  {'answers': [{'text': '파우스트', 'answer_start': 15}],\n",
       "   'id': '6566518-0-0',\n",
       "   'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'},\n",
       "  {'answers': [{'text': '합창교향곡', 'answer_start': 354}],\n",
       "   'id': '6566518-0-1',\n",
       "   'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'},\n",
       "  {'answers': [{'text': '1839', 'answer_start': 0}],\n",
       "   'id': '5917067-0-0',\n",
       "   'question': '바그너가 파우스트를 처음으로 읽은 년도는?'},\n",
       "  {'answers': [{'text': '파리', 'answer_start': 410}],\n",
       "   'id': '5917067-0-1',\n",
       "   'question': '바그너가 처음 교향곡 작곡을 한 장소는?'},\n",
       "  {'answers': [{'text': '드레스덴', 'answer_start': 534}],\n",
       "   'id': '5917067-0-2',\n",
       "   'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}],\n",
       " 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korquad_train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '또한, 정책 및 사업의 본격 추진을 위한 재원의 확보 방안을 마련해야 한다. 특별법의 하위 법령 또는 관련 법령의 제·개정을 통해 불이행 행위자에 강제 이행 유도를 위한 벌금 또는 ‘오염 원인자 부담’의 원칙에 따른 ‘해양 대기오염 부담금(안)’을 부과할 수 있다. 부담금 징수로 인한 재정 수익분을 항만의 대기환경 개선을 위한 특별회계에서 수용하도록 하고, 예산의 배분에 있어서도 사업의 시급성, 파급 효과 등을 고려하여 사업의 우선순위에 차등을 두어 지원하는 ‘선택과 집중’의 전략적 방법론을 적용해야 한다. 이와 더불어 유관 부처 및 기관 등 주요 정책 행위자들의 업무 역할과 기능, 나아가 의무 및 권한의 범위 등에 대한 명확한 법적 근거 역시 시급하게 마련되어야 한다. 각 부처 및 기관별로 사업을 추진하고 개별성과를 기계적으로 취합하는 현재의 방식보다는, 통합 관리 차원에서 유관 부처·기관 간 공동 연구개발 및 조사, 공공 데이터 차원의 정보 연계·공유 및 활용 등을 통해 정책 및 사업의 효율성을 높이고, 공동의 사회 문제 해결을 위한 시너지의 도출이 필요하다.',\n",
       " 'qas': [{'question': '예산을 분배할 때 사업의 우선 순위를 정해서 차등 지원하는 방법을 뭐라고 하지',\n",
       "   'id': 1063333,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '사업의 우선 순위를 정해서 예산을 차등적으로 나눠주는 방법을 뭐라고 해',\n",
       "   'id': 1063334,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '사업의 우선 순위로 예산을 차등 분배하는 방법을 뭐라고 불러',\n",
       "   'id': 1063335,\n",
       "   'is_impossible': False,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]},\n",
       "  {'question': '뭐가 사업의 크기를 정해서 예산을 차등 지원한다고 하니',\n",
       "   'id': 1063336,\n",
       "   'is_impossible': True,\n",
       "   'answers': [{'text': '선택과 집중', 'answer_start': 260}]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookqa_train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '20180305114044000',\n",
       " 'paragraphs': [{'context': '중국의 한 여성 경찰이 아파트에서 추락하던 3세 아이를 살리고 자신은 혼수상태에 빠졌습니다. 의인(義人)의 소식이 알려지자 각박한 중국 사회에 큰 반향을 일으키고 있습니다. 5일 귀주도시망 등 중국 현지 언론에 따르면 구이저우성 카일리시에 보조 교통 경찰로 일하는 천중핑(49)은 지난달 28일 한 아파트에서 비상 상황이 발생했다는 연락을 받고 현장으로 향했습니다. 도착했을 때 아파트 4층 창문에서 여자 아이가 매달려 있었습니다. 곧이어 아이는 손에 힘이 빠지면서 밑으로 추락했습니다. 천중핑과 다른 세명의 이웃들이 달려갔습니다. 그리고 아이는 바닥이 아니라 천중핑의 팔에 떨어졌습니다. 중간 비막이 천막 때문에 속도가 줄기는 했지만 추락의 충격은 천중핑이 고스란히 감당해야 했습니다. 아이는 즉시 병원으로 옮겨져 치료를 받았습니다. 다리 골절로 그리 심각한 상황은 아니라고 합니다. 하지만 생명의 은인이자 영웅은 커다란 댓가를 치러야 했다. 뇌출혈로 인한 의식불명 상태에 빠진 것이다. 다행히 이틀 간의 코마 상태 이후 의식을 회복해 지난 2일부터 중환자실에서 치료를 받고 있습니다. 아이는 열쇠공이 문을 따는 소리에 겁을 먹고 창문 밖으로 도망을 치려다 사고를 당한 것으로 전해졌습니다. 아이가 잠든 사이 돌보던 아이의 할머니가 쓰레기를 버리러 나갔다가 문이 잠기는 바람에 열쇠공을 불렀던 것입니다. 아이의 엄마는 “천중핑의 도움이 없었다면 아이는 죽었을 것”이라며 딸을 구해준 천중핑에게 감사의 뜻을 전했습다. 카일리시 정부 대표와 공안부 관계자들도 천중핑이 입원한 병원을 찾아 위로하고 회복될때까지 도움을 아끼지 않겠다고 밝혔습니다. 천중핑의 선행 사실을 접한 중국 기업 알리바바도 ‘중국의 좋은 이웃상’과 함께 상금 1만 위안(약 170만원)을 수여하기로 했습니다. [아직 살만한 세상]은 점점 각박해지는 세상에 희망과 믿음을 주는 이들의 이야기입니다. 힘들고 지칠 때 아직 살만한 세상을 만들어가는 ‘아살세’ 사람들의 목소리를 들어보세요. 따뜻한 세상을 꿈꾸는 독자 여러분의 제보를 기다립니다. 맹경환 기자 khmaeng@kmib.co.kr',\n",
       "   'qas': [{'question': '중국에서 아파트에서 추락하던 3세 아이를 살리고 자신은 혼수상태에 빠진 사람은 누구야?',\n",
       "     'answers': [{'answer_start': 134, 'text': '보조 교통 경찰로 일하는 천중핑'}],\n",
       "     'id': 'c1_151305-1',\n",
       "     'classtype': 'work_who'},\n",
       "    {'question': '천중핑씨가 추락하는 아이를 구하고 뇌출혈로 인한 의식불명 상태에  빠진건 언제야?',\n",
       "     'answers': [{'answer_start': 157, 'text': '지난달 28일'}],\n",
       "     'id': 'c1_151306-1',\n",
       "     'classtype': 'work_when'},\n",
       "    {'question': '천중핑씨가 보조 교통 경찰로 일하는 곳은 어디야?',\n",
       "     'answers': [{'answer_start': 122, 'text': '구이저우성 카일리시'}],\n",
       "     'id': 'c1_151307-1',\n",
       "     'classtype': 'work_where'},\n",
       "    {'question': '천중핑의 선행 사실을 접한 중국 기업 알리바바는 무엇을 하기로 했어?',\n",
       "     'answers': [{'answer_start': 835,\n",
       "       'text': '‘중국의 좋은 이웃상’과 함께 상금 1만 위안(약 170만원)을 수여'}],\n",
       "     'id': 'c1_151308-1',\n",
       "     'classtype': 'work_what'},\n",
       "    {'question': '천중핑씨의 상태는 어때?',\n",
       "     'answers': [{'answer_start': 502,\n",
       "       'text': '이틀 간의 코마 상태 이후 의식을 회복해 지난 2일부터 중환자실에서 치료를 받고 있습니다'}],\n",
       "     'id': 'c1_151309-1',\n",
       "     'classtype': 'work_how'},\n",
       "    {'question': '3세 아이는 왜 4층 창문에 매달려 있었어?',\n",
       "     'answers': [{'answer_start': 557,\n",
       "       'text': '열쇠공이 문을 따는 소리에 겁을 먹고 창문 밖으로 도망을 치려다'}],\n",
       "     'id': 'c1_151310-1',\n",
       "     'classtype': 'work_why'},\n",
       "    {'question': '열쇠공은 왜 잠긴 문을 따려고 한거야?',\n",
       "     'answers': [{'answer_start': 612,\n",
       "       'text': '아이가 잠든 사이 돌보던 아이의 할머니가 쓰레기를 버리러 나갔다가 문이 잠기는 바람에 열쇠공을 불렀던 것'}],\n",
       "     'id': 'c1_151311-1',\n",
       "     'classtype': 'work_why'}]}],\n",
       " 'source': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_all['data'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = [data['source'] for data in news_all['data']]\n",
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT/과학 \t 5017\n",
      "스포츠 \t 5021\n",
      "생활 \t 5765\n",
      "사회 \t 5625\n",
      "경제 \t 3981\n",
      "연예 \t 5091\n",
      "미용/건강 \t 4272\n",
      "정치 \t 7317\n",
      "문화 \t 5225\n"
     ]
    }
   ],
   "source": [
    "src_id2cls = {1: '정치', 2: '경제', 3: '사회', 4: '생활', 5: 'IT/과학', 6: '연예', 7: '스포츠', 8: '문화', 9: '미용/건강'}\n",
    "\n",
    "for k, v in dict(Counter(sources)).items():\n",
    "    print(src_id2cls[k], '\\t', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(file_path, has_src=False):\n",
    "  with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "  \n",
    "  data_rows = []\n",
    "  for doc in tqdm(data['data']):\n",
    "    if has_src:\n",
    "      source = doc['source']\n",
    "    else:\n",
    "      source = 'all'\n",
    "    for paragraph in doc['paragraphs']:\n",
    "      context = paragraph['context']\n",
    "      for question_and_answers in paragraph['qas']:\n",
    "        question = question_and_answers['question']\n",
    "        answers = question_and_answers['answers']\n",
    "        \n",
    "        for answer in answers:\n",
    "          answer_text = answer['text']\n",
    "          answer_start = answer['answer_start']\n",
    "          answer_end = answer_start + len(answer_text)\n",
    "\n",
    "          data_rows.append({\n",
    "            'question': question,\n",
    "            'context': context,\n",
    "            'answer_text': answer_text,\n",
    "            'answer_start': answer_start,\n",
    "            'answer_end': answer_end,\n",
    "            'source': source\n",
    "        })\n",
    "  return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:00<00:00, 19402.77it/s]\n",
      "100%|██████████| 5368/5368 [00:00<00:00, 6118.00it/s]\n",
      "100%|██████████| 47314/47314 [00:00<00:00, 182909.15it/s]\n"
     ]
    }
   ],
   "source": [
    "korquad_train = extract_questions_and_answers(os.path.join(KORQUAD_PATH, KORQUAD_TRAIN))\n",
    "bookqa_train = extract_questions_and_answers(os.path.join(BOOKQA_PATH, BOOKQA_TRAIN))\n",
    "news_all = extract_questions_and_answers(os.path.join(NEWSQA_PATH, NEWS_ALL), has_src=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Vizualization of BERT output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9681"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korquad_contexts = []\n",
    "for data in korquad_train['data']:\n",
    "    for context_qas in data['paragraphs']:\n",
    "        context = context_qas['context']\n",
    "        korquad_contexts.append(context)\n",
    "\n",
    "len(korquad_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5017\n",
      "7 5021\n",
      "4 5765\n",
      "3 5625\n",
      "2 3981\n",
      "6 5091\n",
      "9 4272\n",
      "1 7317\n",
      "8 5225\n"
     ]
    }
   ],
   "source": [
    "news_contexts = defaultdict(list)\n",
    "for data in news_all['data']:\n",
    "    source = data['source']\n",
    "    for context_qas in data['paragraphs']:\n",
    "        context = context_qas['context']\n",
    "        news_contexts[source].append(context)\n",
    "\n",
    "for k, v in news_contexts.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_contexts = []\n",
    "for data in bookqa_train['data']:\n",
    "    for context_qas in data['paragraphs']:\n",
    "        context = context_qas['context']\n",
    "        book_contexts.append(context)\n",
    "len(book_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"klue/bert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=64):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47314"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_contexts_list = list(chain.from_iterable([v for k, v in news_contexts.items()]))\n",
    "news_labels_list = list(chain.from_iterable([[k] * len(v) for k, v in news_contexts.items()]))\n",
    "len(news_contexts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11829/11829 [18:21<00:00, 10.74it/s]\n"
     ]
    }
   ],
   "source": [
    "cls_token, train_masks, train_ys = torch.zeros(0, 768), torch.zeros(0, 512), torch.zeros(0,1)\n",
    "span_length = 4\n",
    "with torch.no_grad():\n",
    "    for r in tqdm(batch(range(len(news_contexts_list)), n=span_length), total=(len(news_contexts_list)//span_length)+1):\n",
    "        batch_contexts, batch_labels = news_contexts_list[r[0]:r[-1]], news_labels_list[r[0]:r[-1]]\n",
    "        encodings = tokenizer(batch_contexts,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt')\n",
    "        output = model(encodings['input_ids'].to(device), encodings['attention_mask'].to(device), output_hidden_states=True, return_dict=True)\n",
    "        cls_token = torch.cat([cls_token, output.last_hidden_state[:,0,:].cpu()])\n",
    "        #train_masks = torch.cat([train_masks, encodings['attention_mask'].cpu()])\n",
    "        train_ys = torch.cat([train_ys, torch.tensor(batch_labels).cpu().view(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5.0: 3763,\n",
       "         7.0: 3766,\n",
       "         4.0: 4324,\n",
       "         3.0: 4218,\n",
       "         2.0: 2986,\n",
       "         6.0: 3818,\n",
       "         9.0: 3204,\n",
       "         1.0: 5488,\n",
       "         8.0: 3918})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_ys.squeeze().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ir/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "/home/ubuntu/anaconda3/envs/ir/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.076500</td>\n",
       "      <td>-1.291866</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.110722</td>\n",
       "      <td>0.209754</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-47.282864</td>\n",
       "      <td>-7.722584</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.982437</td>\n",
       "      <td>16.556686</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.848850</td>\n",
       "      <td>15.162859</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35480</th>\n",
       "      <td>-0.863062</td>\n",
       "      <td>-35.764702</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35481</th>\n",
       "      <td>10.867757</td>\n",
       "      <td>-56.259861</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35482</th>\n",
       "      <td>11.565045</td>\n",
       "      <td>74.315536</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35483</th>\n",
       "      <td>62.433628</td>\n",
       "      <td>-13.470034</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35484</th>\n",
       "      <td>18.625132</td>\n",
       "      <td>-8.998246</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35485 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x          y  label\n",
       "0      11.076500  -1.291866      5\n",
       "1     -17.110722   0.209754      5\n",
       "2     -47.282864  -7.722584      5\n",
       "3      41.982437  16.556686      5\n",
       "4      43.848850  15.162859      5\n",
       "...          ...        ...    ...\n",
       "35480  -0.863062 -35.764702      8\n",
       "35481  10.867757 -56.259861      8\n",
       "35482  11.565045  74.315536      8\n",
       "35483  62.433628 -13.470034      8\n",
       "35484  18.625132  -8.998246      8\n",
       "\n",
       "[35485 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_reducer = TSNE(n_components=2)\n",
    "# averaged_layer_hidden_states = torch.div(layer_hidden_states.sum(dim=1), train_masks.sum(dim=1,keepdim=True))\n",
    "layer_dim_reduced_vectors = dim_reducer.fit_transform(cls_token.detach().numpy())\n",
    "df = pd.DataFrame.from_dict({'x':layer_dim_reduced_vectors[:,0],'y':layer_dim_reduced_vectors[:,1],'label':train_ys.squeeze()})\n",
    "df.label = df.label.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.076500</td>\n",
       "      <td>-1.291866</td>\n",
       "      <td>IT/과학</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.110722</td>\n",
       "      <td>0.209754</td>\n",
       "      <td>IT/과학</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-47.282864</td>\n",
       "      <td>-7.722584</td>\n",
       "      <td>IT/과학</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.982437</td>\n",
       "      <td>16.556686</td>\n",
       "      <td>IT/과학</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.848850</td>\n",
       "      <td>15.162859</td>\n",
       "      <td>IT/과학</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y  label\n",
       "0  11.076500  -1.291866  IT/과학\n",
       "1 -17.110722   0.209754  IT/과학\n",
       "2 -47.282864  -7.722584  IT/과학\n",
       "3  41.982437  16.556686  IT/과학\n",
       "4  43.848850  15.162859  IT/과학"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].apply(lambda x: src_id2cls[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib' from '/home/ubuntu/anaconda3/envs/ir/lib/python3.7/site-packages/matplotlib/__init__.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "path = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "fontprop = fm.FontProperties(fname=path, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "# plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.scatter(x=df['x'], y=df['y'], s=df['label'], fontproperties=fontprop)\n",
    "# sns.scatterplot(data=df, x='x',y='y',hue='label', fontproperties=fontprop)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# path = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "# fontprop = fm.FontProperties(fname=path, size=18)\n",
    "\n",
    "# plt.plot(range(50), data, 'r')\n",
    "# plt.title('가격변동 추이', fontproperties=fontprop)\n",
    "# plt.ylabel('가격', fontproperties=fontprop)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../result/news_tsne_noavg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_contexts = korquad_contexts + list(chain.from_iterable([v for k, v in news_contexts.items()]))\n",
    "total_labels = [0] * len(korquad_contexts) + list(chain.from_iterable([[k] * len(v) for k, v in news_contexts.items()]))\n",
    "len(total_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(total_contexts,\n",
    "                      max_length=512,\n",
    "                      truncation=True,\n",
    "                      padding='max_length',\n",
    "                      return_attention_mask=True,\n",
    "                      return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['또한, 정책 및 사업의 본격 추진을 위한 재원의 확보 방안을 마련해야 한다. 특별법의 하위 법령 또는 관련 법령의 제·개정을 통해 불이행 행위자에 강제 이행 유도를 위한 벌금 또는 ‘오염 원인자 부담’의 원칙에 따른 ‘해양 대기오염 부담금(안)’을 부과할 수 있다. 부담금 징수로 인한 재정 수익분을 항만의 대기환경 개선을 위한 특별회계에서 수용하도록 하고, 예산의 배분에 있어서도 사업의 시급성, 파급 효과 등을 고려하여 사업의 우선순위에 차등을 두어 지원하는 ‘선택과 집중’의 전략적 방법론을 적용해야 한다. 이와 더불어 유관 부처 및 기관 등 주요 정책 행위자들의 업무 역할과 기능, 나아가 의무 및 권한의 범위 등에 대한 명확한 법적 근거 역시 시급하게 마련되어야 한다. 각 부처 및 기관별로 사업을 추진하고 개별성과를 기계적으로 취합하는 현재의 방식보다는, 통합 관리 차원에서 유관 부처·기관 간 공동 연구개발 및 조사, 공공 데이터 차원의 정보 연계·공유 및 활용 등을 통해 정책 및 사업의 효율성을 높이고, 공동의 사회 문제 해결을 위한 시너지의 도출이 필요하다.',\n",
       " '최근 국민적 관심에 힘입어 미세먼지 등 대기오염 문제가 커다란 사회적 현안으로 등장하고 있다. 특히 대형 선박의 입출항 및 정박은 물론 하역, 운송 등의 활동이 이루어지는 항만은 오랜 시간 동안 배출관리의 사각지대에 남아있었으나, 항만 인근 지역은 물론 국가 대기오염의 주된 원인으로 지목되고 있다. 해마다 환경부가 발표하는 ‘국가 대기오염물질 배출량 통계’에서 국가 전체 배출량 중 선박에 의한 배출 비중이 매우 높게 나타나고 있으며 주요 항만을 포함하는 지역의 선박 배출량 규모 역시 매우 크게 나타나고 있기 때문이다. 항만의 배출저감 및 관리를 위한 대책이 시급한 이유가 여기에 있다. 이에, 환경부는 미세먼지 및 미세먼지 생성물질의 배출을 저감하고 종합적으로 관리하기 위한 「미세먼지 저감 및 관리에 관한 특별법」을 제정하여 지난 2월부터 시행 중이다. 해양수산부 역시 항만 및 인근 영향지역의 미세먼지 배출과 오염을 집중 관리함과 동시에 환경친화적 선박을 확대하고 친환경 항만 인프라를 구축하기 위한 「항만지역 등 대기질 개선에 관한 특별법안」을 국회와 공조하여 지난 3월 13일 제정하였으며 2020년 1월부터 시행 예정이다.',\n",
       " '특별법의 제정으로 항만의 대기오염을 방지·저감하기 위한 정책 및 사업 추진의 법적 근거는 마련되었으나, 하위 법령 및 관련 법령의 제·개정을 통하여 특별법 이행을 위한 구체적 사항들을 규정하는 동시에 항만지역의 대기질을 획기적으로 개선하기 위한 사업을 발굴하고 이를 추진하기 위한 유관 부처·기관 간 협의, 재원 마련을 위한 작업 등이 신속하게 이루어져야 하는 시점이다. 과거 정부 및 항만당국, 유관 기관들의 전 방위적인 대응에도 불구하고 항만에 대한 대기오염 저감 정책 및 사업 추진에 따른 성과가 뚜렷하게 나타나지 않았으며, 오히려 국민이 체감하는 대기 환경의 질은 더욱 악화되고 있는 것으로 보고되고 있다. 새로이 제정된 특별법을 견실하게 이행하고 나아가 항만지역 대기질을 획기적으로 개선하기 위해서는, 이후 관련 법령의 제·개정, 그리고 예산을 확보·집행하는 과정에서 기존 제도의 문제점 및 이를 개선하기 위한 방안이 충분히 고려되어야 한다.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = bookqa_train['data'][0]['paragraphs'][0]['context']\n",
    "# test\n",
    "samples = [context['context'] for context in bookqa_train['data'][0]['paragraphs']]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/891.546875 [00:40<9:57:17, 40.24s/it]"
     ]
    }
   ],
   "source": [
    "train_masks, train_ys = torch.zeros(0, 512), torch.zeros(0,1)\n",
    "train_hidden_states = None\n",
    "span_length = 64\n",
    "for r in tqdm(batch(range(len(total_labels)), n=span_length), total=(len(total_labels)//span_length)+1):\n",
    "    batch_contexts, batch_labels = total_contexts[r[0]:r[-1]], total_labels[r[0]:r[-1]]\n",
    "    encodings = tokenizer(batch_contexts,\n",
    "                        max_length=512,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt')\n",
    "    output = model(encodings['input_ids'].to(device), encodings['attention_mask'].to(device), output_hidden_states=True, return_dict=True)\n",
    "    hidden_states = output.hidden_states[1: ]\n",
    "    train_masks = torch.cat([train_masks, encodings['attention_mask'].cpu()])\n",
    "    train_ys = torch.cat([train_ys, torch.tensor(batch_labels).cpu().view(-1,1)])\n",
    "    if type(train_hidden_states) == type(None):\n",
    "        train_hidden_states = tuple(layer_hidden_states.cpu() for layer_hidden_states in hidden_states)\n",
    "    else:\n",
    "        train_hidden_states = tuple(torch.cat([layer_hidden_state_all, layer_hidden_state_batch.cpu()])for layer_hidden_state_all, layer_hidden_state_batch in zip(train_hidden_states, hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoding = tokenizer(samples,\n",
    "                          max_length=512,\n",
    "                          truncation=True,\n",
    "                          padding='max_length',\n",
    "                          return_attention_mask=True,\n",
    "                          return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks, train_ys = torch.zeros(0, 512), torch.zeros(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(test_encoding['input_ids'], test_encoding['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "hidden_states = output.hidden_states[1:]\n",
    "train_hidden_states = tuple(layer_hidden_states.cpu() for layer_hidden_states in hidden_states)\n",
    "train_masks = torch.cat([train_masks, test_encoding['attention_mask'].cpu()])\n",
    "train_ys = torch.cat([train_ys, torch.tensor([0, 0, 0]).cpu().view(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37499854,  0.30711803, -0.22767217, ..., -0.27081186,\n",
       "        -0.04755132,  0.52785164],\n",
       "       [-0.15950038, -0.1044018 , -0.43394968, ...,  0.67631674,\n",
       "         0.20772135,  0.32073516],\n",
       "       [-0.36717808,  0.33571374, -0.3536198 , ...,  0.58097416,\n",
       "         0.05888833,  0.15914038]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state[:,0,:].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3885/3464793669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdim_reducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maveraged_layer_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlayer_dim_reduced_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_reducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maveraged_layer_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer_dim_reduced_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer_dim_reduced_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "dim_reducer = TSNE(n_components=2)\n",
    "layer_hidden_states = train_hidden_states[11]\n",
    "averaged_layer_hidden_states = torch.div(layer_hidden_states.sum(dim=1), train_masks.sum(dim=1,keepdim=True))\n",
    "layer_dim_reduced_vectors = dim_reducer.fit_transform(averaged_layer_hidden_states.detach().numpy())\n",
    "df = pd.DataFrame.from_dict({'x':layer_dim_reduced_vectors[:,0],'y':layer_dim_reduced_vectors[:,1],'label':train_ys.squeeze()})\n",
    "df.label = df.label.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df,x='x',y='y',hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path, is_source=False, source_no=\"all\"):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    qa_data = defaultdict(list)\n",
    "    for doc in data['data']:\n",
    "        if is_source:\n",
    "            source = doc['source']\n",
    "        else:\n",
    "            source = \"all\"\n",
    "        filtered = True if source == source_no else False\n",
    "        if filtered:\n",
    "            for paragraph in doc['paragraphs']:\n",
    "                context = paragraph['context'].replace('\\u200b', '')\n",
    "                for question_and_answers in paragraph['qas']:\n",
    "                    is_impossible = question_and_answers['is_impossible'] if 'is_impossible' in question_and_answers else None\n",
    "                    if not is_impossible:\n",
    "                        question = question_and_answers['question']\n",
    "                        answers = question_and_answers['answers']    \n",
    "                        for answer in answers:\n",
    "                            qa_data['context'].append(context)\n",
    "                            qa_data['question'].append(question)\n",
    "                            qa_data['answers'].append(answer)\n",
    "                            if is_source:\n",
    "                                qa_data['source'].append(source)\n",
    "    return qa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ubuntu/workspace/kaist.ir/qa/data'\n",
    "squad_train_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_train.json'))\n",
    "squad_valid_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hub_data = read_file(os.path.join(data_path, 'newsqa/news_train_all_10.json'))\n",
    "valid_hub_data = read_file(os.path.join(data_path, 'newsqa/news_test_all_10.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_valid_data = defaultdict(list)\n",
    "for question, context, answer in zip(squad_valid_data['question'], squad_valid_data['context'], squad_valid_data['answers']):\n",
    "    if len(context) > 1000:\n",
    "        test_valid_data['question'].append(question)\n",
    "        test_valid_data['context'].append(context)\n",
    "        test_valid_data['answers'].append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_valid_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 14/233 [00:00<00:00, 419.48it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_data = test_valid_data\n",
    "max_len=512\n",
    "doc_stride=0\n",
    "\n",
    "encodings = []\n",
    "long_cnt = 0\n",
    "in_answer = 0\n",
    "for context, question in tqdm(zip(qa_data['context'], qa_data['question']), total=len(qa_data['context'])):\n",
    "    encoding = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_len,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    if len(encoding['input_ids']) > 1:\n",
    "        cnt += 1\n",
    "    sample_mapping = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "\n",
    "    encoding[\"start_positions\"] = []\n",
    "    encoding[\"end_positions\"] = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = encoding[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = encoding.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = qa_data['answers'][sample_index]\n",
    "        start_char = answers['answer_start']\n",
    "        end_char = start_char + len(answers['text'])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            encoding[\"start_positions\"].append(cls_index)\n",
    "            encoding[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            encoding[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            encoding[\"end_positions\"].append(token_end_index + 1)\n",
    "    # has_start_idx = [idx for idx, sp in enumerate(encoding['start_positions']) if sp != cls_index]\n",
    "    # has_end_idx = [idx for idx, ep in enumerate(encoding['end_positions']) if ep != cls_index]\n",
    "\n",
    "    # if has_start_idx == has_end_idx and len(has_start_idx) != 0:\n",
    "    #     has_idx = has_start_idx[0]\n",
    "    #     ft_encodings = {k: v[has_idx] for k, v in encoding.items()}\n",
    "    #     encodings.append(ft_encodings)\n",
    "    # if len(encoding['input_ids']) > 2:\n",
    "    #     break\n",
    "    # # del encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5774/5774 [00:06<00:00, 873.35it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_data = squad_valid_data\n",
    "max_len=512\n",
    "doc_stride=0\n",
    "\n",
    "encodings = []\n",
    "long_cnt = 0\n",
    "in_answer = 0\n",
    "for context, question, answers in tqdm(zip(qa_data['context'], qa_data['question'], qa_data['answers']), total=len(qa_data['context'])):\n",
    "    encoding = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=max_len,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "\n",
    "    encoding[\"start_positions\"] = []\n",
    "    encoding[\"end_positions\"] = []\n",
    "    \n",
    "    input_ids = encoding['input_ids']\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    sequence_ids = encoding.sequence_ids(0)\n",
    "    \n",
    "    start_char = answers['answer_start']\n",
    "    end_char = start_char + len(answers['text'])\n",
    "\n",
    "    token_start_index = 0\n",
    "    while sequence_ids[token_start_index] != 1:\n",
    "        token_start_index += 1\n",
    "\n",
    "    token_end_index = len(input_ids) - 1\n",
    "    while sequence_ids[token_end_index] != 1:\n",
    "        token_end_index -= 1\n",
    "    \n",
    "    if not (offset_mapping[token_start_index][0] <= start_char and offset_mapping[token_end_index][1] >= end_char):\n",
    "        encoding[\"start_positions\"].append(cls_index)\n",
    "        encoding[\"end_positions\"].append(cls_index)\n",
    "        \n",
    "    else:\n",
    "        while token_start_index < len(offset_mapping) and offset_mapping[token_start_index][0] <= start_char:\n",
    "            token_start_index += 1\n",
    "        encoding[\"start_positions\"].append(token_start_index - 1)\n",
    "        while offset_mapping[token_end_index][1] >= end_char:\n",
    "            token_end_index -= 1\n",
    "        encoding[\"end_positions\"].append(token_end_index + 1)\n",
    "        in_answer += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5717\n"
     ]
    }
   ],
   "source": [
    "print(in_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 19)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping[token_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 19)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping[token_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n"
     ]
    }
   ],
   "source": [
    "print(token_start_index, token_end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37] [39]\n"
     ]
    }
   ],
   "source": [
    "print(encoding['start_positions'], encoding['end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(file_path):\n",
    "  with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "  \n",
    "  data_rows = []\n",
    "  for doc in data['data']:\n",
    "    for paragraph in doc['paragraphs']:\n",
    "      context = paragraph['context']\n",
    "      for question_and_answers in paragraph['qas']:\n",
    "        question = question_and_answers['question']\n",
    "        answers = question_and_answers['answers']\n",
    "        \n",
    "        for answer in answers:\n",
    "          answer_text = answer['text']\n",
    "          answer_start = answer['answer_start']\n",
    "          answer_end = answer_start + len(answer_text)\n",
    "\n",
    "          data_rows.append({\n",
    "            'question': question,\n",
    "            'context': context,\n",
    "            'answer_text': answer_text,\n",
    "            'answer_start': answer_start,\n",
    "            'answer_end': answer_end\n",
    "        })\n",
    "  return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ubuntu/workspace/kaist.ir/qa/data'\n",
    "kor_train = extract_questions_and_answers(os.path.join(data_path, 'korquad/KorQuAD_v1.0_train.json'))\n",
    "kor_valid = extract_questions_and_answers(os.path.join(data_path, 'korquad/KorQuAD_v1.0_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train = extract_questions_and_answers(os.path.join(data_path, 'newsqa/news_train_all_10.json'))\n",
    "news_valid = extract_questions_and_answers(os.path.join(data_path, 'newsqa/news_test_all_10.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5년 전 93억원 규모였던 시장이 지난해 400억원 규모로 커졌고 5년 뒤에는 15...</td>\n",
       "      <td>\"오늘 전국이 25도 안팎의 날씨를 보였죠. 기온이 26도 정도가 되면 맥주의 매출...</td>\n",
       "      <td>수제맥주협회</td>\n",
       "      <td>768</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>소규모 맥주 면허는 언제 102개로 늘었는가?</td>\n",
       "      <td>\"오늘 전국이 25도 안팎의 날씨를 보였죠. 기온이 26도 정도가 되면 맥주의 매출...</td>\n",
       "      <td>지난 3월</td>\n",
       "      <td>749</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>수제맥주를 대부분 다품종 소량 생산하는 이유는?</td>\n",
       "      <td>\"오늘 전국이 25도 안팎의 날씨를 보였죠. 기온이 26도 정도가 되면 맥주의 매출...</td>\n",
       "      <td>양조장이 작아서</td>\n",
       "      <td>549</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>밀맥주는 발효 과정에 무엇을 넣어 만드는 맥주인가?</td>\n",
       "      <td>\"오늘 전국이 25도 안팎의 날씨를 보였죠. 기온이 26도 정도가 되면 맥주의 매출...</td>\n",
       "      <td>국화</td>\n",
       "      <td>505</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6년 전 영국의 경제전문 기자는 어떤 칼럼을 게재했나?</td>\n",
       "      <td>\"오늘 전국이 25도 안팎의 날씨를 보였죠. 기온이 26도 정도가 되면 맥주의 매출...</td>\n",
       "      <td>&amp;한국 맥주는 북한의 대동강맥주보다 맛이 없다&amp;는 내용의 칼럼</td>\n",
       "      <td>959</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220205</th>\n",
       "      <td>경찰은 수사권에 대해 무슨 주장을 했어?</td>\n",
       "      <td>검찰과 경찰의 수사권 갈등이 원만하게 해결되었다더니 문제가 더 꼬여가는 모양이다. ...</td>\n",
       "      <td>이제 자질향상이 이루어졌으니 수사권 일부를 달라는 주장이었다.</td>\n",
       "      <td>847</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220206</th>\n",
       "      <td>검찰이 수사지휘권을 더 확고하게 다진 반면, 경찰은 어떤 평가를 했어?</td>\n",
       "      <td>검찰과 경찰의 수사권 갈등이 원만하게 해결되었다더니 문제가 더 꼬여가는 모양이다. ...</td>\n",
       "      <td>지금까지 누려온 권한을 명문화시킨 명분상의 이익을 얻었다는 평가였다.</td>\n",
       "      <td>1036</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220216</th>\n",
       "      <td>JYJ는 ‘2011 JYJ WORLD TOUR CONCERT’를 성공리에 마감했으며...</td>\n",
       "      <td>[IMG1]JYJ의 김재중이 ‘시티헌터’ 후속으로 SBS에서 방송되는 수목극 ‘보스...</td>\n",
       "      <td>오는 26일 일요일 저녁 7시 광주광역시 염주종합체육관에서 앵콜 콘서트를 가진다.</td>\n",
       "      <td>586</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220228</th>\n",
       "      <td>사우디아라비아의 상업영화 상영 허용 후, 앞으로 무엇을 계획하고 있나요?</td>\n",
       "      <td>\"사우디아라비아가 35년 만에 상업영화 상영을 허용했습니다. 보수 이슬람 정책의 빗...</td>\n",
       "      <td>2030년까지 복합상영관 300곳을 새로 짓고 3만 개의 일자리를 창출할 계획</td>\n",
       "      <td>511</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220229</th>\n",
       "      <td>영화 부서 관계자는 사우디아라비에 영화관이 부족한 것을 어떻게 해결 하기로 했나요?</td>\n",
       "      <td>\"사우디아라비아가 35년 만에 상업영화 상영을 허용했습니다. 보수 이슬람 정책의 빗...</td>\n",
       "      <td>대체 시설을 영화관으로 개조해 적극 활용</td>\n",
       "      <td>479</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58692 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  ... answer_end\n",
       "15      5년 전 93억원 규모였던 시장이 지난해 400억원 규모로 커졌고 5년 뒤에는 15...  ...        774\n",
       "16                              소규모 맥주 면허는 언제 102개로 늘었는가?  ...        754\n",
       "19                             수제맥주를 대부분 다품종 소량 생산하는 이유는?  ...        557\n",
       "21                           밀맥주는 발효 과정에 무엇을 넣어 만드는 맥주인가?  ...        507\n",
       "22                         6년 전 영국의 경제전문 기자는 어떤 칼럼을 게재했나?  ...        993\n",
       "...                                                   ...  ...        ...\n",
       "220205                             경찰은 수사권에 대해 무슨 주장을 했어?  ...        881\n",
       "220206            검찰이 수사지휘권을 더 확고하게 다진 반면, 경찰은 어떤 평가를 했어?  ...       1074\n",
       "220216  JYJ는 ‘2011 JYJ WORLD TOUR CONCERT’를 성공리에 마감했으며...  ...        631\n",
       "220228           사우디아라비아의 상업영화 상영 허용 후, 앞으로 무엇을 계획하고 있나요?  ...        554\n",
       "220229     영화 부서 관계자는 사우디아라비에 영화관이 부족한 것을 어떻게 해결 하기로 했나요?  ...        501\n",
       "\n",
       "[58692 rows x 5 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train[news_train['answer_end'] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "        news_train.loc[220206, 'question'],\n",
    "        news_train.loc[220206, 'context'],\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "\n",
    "encoding[\"start_positions\"] = []\n",
    "encoding[\"end_positions\"] = []\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "sequence_ids = encoding.sequence_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "token_end_index = len(input_ids) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 510\n"
     ]
    }
   ],
   "source": [
    "print(token_start_index, token_end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036 1074\n"
     ]
    }
   ],
   "source": [
    "start_char = news_train.loc[220206, 'answer_start']\n",
    "end_char = news_train.loc[220206, 'answer_end']\n",
    "print(start_char, end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 939)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping[token_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "if not (offset_mapping[token_start_index][0] <= start_char and offset_mapping[token_end_index][1] >= end_char):\n",
    "    encoding[\"start_positions\"].append(cls_index)\n",
    "    encoding[\"end_positions\"].append(cls_index)\n",
    "    print('y')\n",
    "    \n",
    "else:\n",
    "    while token_start_index < len(offset_mapping) and offset_mapping[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    encoding[\"start_positions\"].append(token_start_index - 1)\n",
    "    while offset_mapping[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    encoding[\"end_positions\"].append(token_end_index + 1)\n",
    "    print('n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = Pororo(task=\"pos\", lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9606\n",
      "173922\n",
      "45464\n"
     ]
    }
   ],
   "source": [
    "korquad_contexts = list(set(korquad_train['context']))\n",
    "book_contexts = list(set(bookqa_train['context']))\n",
    "news_contexts = list(set(news_all['context']))\n",
    "\n",
    "print(len(korquad_contexts))\n",
    "print(len(book_contexts))\n",
    "print(len(news_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4110\n"
     ]
    }
   ],
   "source": [
    "ft_news_contexts = list(set(news_all[news_all['source'] == 9]['context']))\n",
    "print(len(ft_news_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNN(contexts: List[str], min: int):\n",
    "    nn_contexts = []\n",
    "    all_nns = []\n",
    "    for context in tqdm(contexts, total=len(contexts)):\n",
    "        pos_result = pos(context)\n",
    "        tmp = []\n",
    "        for result in pos_result:\n",
    "            if len(result[0]) > 1 and result[1] in ['NNG', 'NNP']:\n",
    "                all_nns.append(result[0])\n",
    "                tmp.append(result[0])\n",
    "        nn_contexts.append(tmp)\n",
    "    nns = Counter(all_nns)\n",
    "    ft_nn_contexts = []\n",
    "    for context in nn_contexts:\n",
    "        tmp = []\n",
    "        for nn in context:\n",
    "            if nns[nn] > min:\n",
    "                tmp.append(nn)\n",
    "        ft_nn_contexts.append(' '.join(tmp))\n",
    "\n",
    "    return ft_nn_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9606/9606 [00:26<00:00, 365.66it/s]\n"
     ]
    }
   ],
   "source": [
    "korquad_nn_contexts = getNN(korquad_contexts, min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45464/45464 [03:43<00:00, 203.66it/s]\n"
     ]
    }
   ],
   "source": [
    "news_nn_contexts = getNN(news_contexts, min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173922/173922 [07:54<00:00, 366.50it/s]\n"
     ]
    }
   ],
   "source": [
    "book_nn_contexts = getNN(book_contexts, min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4110/4110 [00:20<00:00, 196.90it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_news_nn_contexts = getNN(ft_news_contexts, min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopN(corpus: List[str], method: str, n: int):\n",
    "    result = dict()\n",
    "    if method == 'common':\n",
    "        all_contexts = []\n",
    "        for c in corpus:\n",
    "            all_contexts.append(c.split())\n",
    "        commons = Counter(list(chain.from_iterable(all_contexts))).most_common(n)\n",
    "        for common in commons:\n",
    "            result[common[0]] = common[1]\n",
    "    elif method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        features = np.array(vectorizer.get_feature_names_out())\n",
    "        sorting = np.argsort(X.toarray()).flatten()[::-1]\n",
    "        top_n = features[sorting][:n]\n",
    "        scores = np.sort(X.data)[:-(n+1):-1]\n",
    "        for word, score in zip(top_n, scores):\n",
    "            result[word] = score    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>병원</td>\n",
       "      <td>3622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>치료</td>\n",
       "      <td>2643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>건강</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대통령</td>\n",
       "      <td>2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>환자</td>\n",
       "      <td>2506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>의료</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>경우</td>\n",
       "      <td>2229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>기자</td>\n",
       "      <td>2147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>조사</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>서울</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>문제</td>\n",
       "      <td>1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>연구</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>의원</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>의사</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>관계</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>정부</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>위원</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>지난</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>수술</td>\n",
       "      <td>1699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>결과</td>\n",
       "      <td>1695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0    병원  3622\n",
       "1    치료  2643\n",
       "2    건강  2607\n",
       "3   대통령  2560\n",
       "4    환자  2506\n",
       "5    의료  2432\n",
       "6    경우  2229\n",
       "7    기자  2147\n",
       "8    조사  2028\n",
       "9    서울  1946\n",
       "10   문제  1908\n",
       "11   연구  1896\n",
       "12   의원  1852\n",
       "13   의사  1827\n",
       "14   관계  1812\n",
       "15   정부  1804\n",
       "16   위원  1758\n",
       "17   지난  1717\n",
       "18   수술  1699\n",
       "19   결과  1695"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = getTopN(ft_news_nn_contexts, method='common', n=20)\n",
    "pd.DataFrame([[k, v] for k,v in result.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대통령</td>\n",
       "      <td>17776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>의원</td>\n",
       "      <td>11140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>회담</td>\n",
       "      <td>8842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>위원</td>\n",
       "      <td>8765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>대표</td>\n",
       "      <td>8045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>북한</td>\n",
       "      <td>7978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>미국</td>\n",
       "      <td>6654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>정부</td>\n",
       "      <td>6532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>민주</td>\n",
       "      <td>6165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>정상</td>\n",
       "      <td>6035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>후보</td>\n",
       "      <td>5839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>문제</td>\n",
       "      <td>5687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>기자</td>\n",
       "      <td>5376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>국회</td>\n",
       "      <td>5229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>검찰</td>\n",
       "      <td>5043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>청와대</td>\n",
       "      <td>4864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>관계</td>\n",
       "      <td>4702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>정치</td>\n",
       "      <td>4656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>선거</td>\n",
       "      <td>4548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>조사</td>\n",
       "      <td>4517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0   대통령  17776\n",
       "1    의원  11140\n",
       "2    회담   8842\n",
       "3    위원   8765\n",
       "4    대표   8045\n",
       "5    북한   7978\n",
       "6    미국   6654\n",
       "7    정부   6532\n",
       "8    민주   6165\n",
       "9    정상   6035\n",
       "10   후보   5839\n",
       "11   문제   5687\n",
       "12   기자   5376\n",
       "13   국회   5229\n",
       "14   검찰   5043\n",
       "15  청와대   4864\n",
       "16   관계   4702\n",
       "17   정치   4656\n",
       "18   선거   4548\n",
       "19   조사   4517"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[k, v] for k,v in result.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getTopN(news_nn_contexts, method='tfidf', n=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "korquad_X = vectorizer.fit_transform(korquad_nn_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pororo/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "features = np.array(vectorizer.get_feature_names_out())\n",
    "sorting = np.argsort(korquad_X.toarray()).flatten()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (/home/ubuntu/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:301742)",
      "at S.execute (/home/ubuntu/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:300732)",
      "at S.start (/home/ubuntu/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:296408)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/ubuntu/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (/home/ubuntu/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "top_n = features[sorting][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95003123, 0.94446449, 0.92270358, 0.91227676, 0.90834932,\n",
       "       0.90688766, 0.90406433, 0.90242535, 0.89777629, 0.89683016])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(korquad_X.data)[:-11:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import hashlib\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import  AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeature():\n",
    "    \"\"\"A single set of features of data\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 input_ids,\n",
    "                 token_type_ids,\n",
    "                 attention_mask,\n",
    "                 start_positions,\n",
    "                 end_positions,\n",
    "                 offset_mapping,\n",
    "                 source=None\n",
    "                 ):\n",
    "        self.id = id\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_positions = start_positions\n",
    "        self.end_positions = end_positions\n",
    "        self.offset_mapping = offset_mapping\n",
    "        self.source = source\n",
    "\n",
    "\n",
    "def read_file(file_path, has_src=False):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    qa_data = defaultdict(list)\n",
    "    for doc in data['data']:\n",
    "        if has_src:\n",
    "            source = doc['source']\n",
    "        else:\n",
    "            source = \"all\"\n",
    "        for paragraph in doc['paragraphs']:\n",
    "            context = paragraph['context'].replace('\\u200b', '')\n",
    "            for question_and_answers in paragraph['qas']:\n",
    "                is_impossible = question_and_answers['is_impossible'] if 'is_impossible' in question_and_answers else None\n",
    "                if not is_impossible:\n",
    "                    question = question_and_answers['question']\n",
    "                    answers = question_and_answers['answers']\n",
    "                    for answer in answers:\n",
    "                        id = question + context\n",
    "                        id = hashlib.shake_256(id.encode()).hexdigest(5)\n",
    "                        qa_data['id'].append(id)\n",
    "                        qa_data['context'].append(context)\n",
    "                        qa_data['question'].append(question)\n",
    "                        qa_data['answers'].append(answer)\n",
    "                        if has_src:\n",
    "                            qa_data['source'].append(source)\n",
    "    return qa_data\n",
    "\n",
    "\n",
    "def convert_to_features(qa_data, tokenizer, max_len, has_src=False):\n",
    "    encodings = []\n",
    "    for idx, (id, context, question, answers) in tqdm(enumerate(zip(qa_data['id'],\n",
    "                                                                    qa_data['context'],\n",
    "                                                                    qa_data['question'],\n",
    "                                                                    qa_data['answers'])), total=len(qa_data['context'])):\n",
    "        encoding = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        encoding['id'] = id\n",
    "        if has_src:\n",
    "            encoding['source'] = qa_data['source'][idx]\n",
    "        else:\n",
    "            encoding['source'] = None\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "        encoding['offset_mapping'] = offset_mapping\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        sequence_ids = encoding.sequence_ids(0)\n",
    "\n",
    "        start_char = answers['answer_start']\n",
    "        end_char = start_char + len(answers['text'])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if offset_mapping[token_start_index][0] <= start_char and offset_mapping[token_end_index][1] >= end_char:\n",
    "            while token_start_index < len(offset_mapping) and offset_mapping[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            encoding[\"start_positions\"] = token_start_index - 1\n",
    "            while offset_mapping[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            encoding[\"end_positions\"] = token_end_index + 1\n",
    "            encodings.append(encoding)\n",
    "\n",
    "    return [InputFeature(enc['id'],\n",
    "                         enc['input_ids'],\n",
    "                         enc['token_type_ids'],\n",
    "                         enc['attention_mask'],\n",
    "                         enc['start_positions'],\n",
    "                         enc['end_positions'],\n",
    "                         enc['offset_mapping'],\n",
    "                         enc['source']) for enc in encodings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ubuntu/workspace/kaist.ir/qa/data'\n",
    "squad_train_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_train.json'))\n",
    "squad_valid_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_dev.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9606\n",
      "60407\n",
      "960\n",
      "5774\n"
     ]
    }
   ],
   "source": [
    "print(len(set(squad_train_data['context'])))\n",
    "print(len(squad_train_data['context']))\n",
    "print(len(set(squad_valid_data['context'])))\n",
    "print(len(squad_valid_data['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_train_data = read_file(os.path.join(data_path, 'bookqa/bookqa_train.json'))\n",
    "book_valid_data = read_file(os.path.join(data_path, 'bookqa/bookqa_valid.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173922\n",
      "630000\n",
      "7343\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "print(len(set(book_train_data['context'])))\n",
    "print(len(book_train_data['context']))\n",
    "print(len(set(book_valid_data['context'])))\n",
    "print(len(book_valid_data['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "uq_contexts = list(set(book_train_data['context']))\n",
    "uq_no = len(set(squad_train_data['context']))\n",
    "\n",
    "rd_book_context = random.sample(uq_contexts, k=uq_no*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630000/630000 [05:59<00:00, 1753.59it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_book_train_data = defaultdict(list)\n",
    "for id, context, question, answers in tqdm(zip(book_train_data['id'],\n",
    "                                         book_train_data['context'],\n",
    "                                         book_train_data['question'],\n",
    "                                         book_train_data['answers']), total=len(book_train_data['id'])):\n",
    "    if context in rd_book_context:\n",
    "        ft_book_train_data['id'].append(id)\n",
    "        ft_book_train_data['context'].append(context)\n",
    "        ft_book_train_data['question'].append(question)\n",
    "        ft_book_train_data['answers'].append(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69056\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_book_train_data['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "valid_uq_contexts = list(set(book_valid_data['context']))\n",
    "valid_uq_no = len(set(squad_valid_data['context']))\n",
    "\n",
    "valid_rd_book_context = random.sample(valid_uq_contexts, k=int(valid_uq_no*1.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35000/35000 [00:00<00:00, 59124.05it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_book_valid_data = defaultdict(list)\n",
    "for id, context, question, answers in tqdm(zip(book_valid_data['id'],\n",
    "                                         book_valid_data['context'],\n",
    "                                         book_valid_data['question'],\n",
    "                                         book_valid_data['answers']), total=len(book_valid_data['id'])):\n",
    "    if context in valid_rd_book_context:\n",
    "        ft_book_valid_data['id'].append(id)\n",
    "        ft_book_valid_data['context'].append(context)\n",
    "        ft_book_valid_data['question'].append(question)\n",
    "        ft_book_valid_data['answers'].append(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248\n",
      "5889\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_rd_book_context))\n",
    "print(len(ft_book_valid_data['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69056/69056 [01:22<00:00, 835.06it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_train_book_features = convert_to_features(ft_book_train_data, tokenizer, max_len=512)\n",
    "ft_valid_book_features = convert_to_features(ft_book_valid_data, tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69056\n",
      "5889\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_train_book_features))\n",
    "print(len(ft_valid_book_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pkl/ft_train_book_features.pkl', 'wb') as f:\n",
    "    pickle.dump(ft_train_book_features, f)\n",
    "\n",
    "with open('../data/pkl/ft_valid_book_features.pkl', 'wb') as f:\n",
    "    pickle.dump(ft_valid_book_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/pkl/ft_train_book_features.pkl', 'rb') as f:\n",
    "#     ft_train_book_features = pickle.load(f)\n",
    "\n",
    "with open('../data/pkl/ft_valid_book_features.pkl', 'rb') as f:\n",
    "    ft_valid_book_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pkl/ft_train_hub_features.pkl', 'rb') as f:\n",
    "    ft_train_hub_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ft_valid_book_features:\n",
    "    if f.start_positions > f.end_positions:\n",
    "        print(f.start_positions, f.end_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter News by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pkl/train_hub_features.pkl', 'rb') as f:\n",
    "    train_hub_features = pickle.load(f)\n",
    "\n",
    "with open('../data/pkl/valid_hub_features.pkl', 'rb') as f:\n",
    "    valid_hub_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 1\n",
    "ft_train_hub_features = [f for f in train_hub_features if f.source==source]\n",
    "ft_valid_hub_features = [f for f in valid_hub_features if f.source==source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32762\n",
      "3360\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_train_hub_features))\n",
    "print(len(ft_valid_hub_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 1 has 32762 features\n",
      "source 2 has 16855 features\n",
      "source 3 has 25444 features\n",
      "source 4 has 25466 features\n",
      "source 5 has 22356 features\n",
      "source 6 has 19996 features\n",
      "source 7 has 22456 features\n",
      "source 8 has 21861 features\n",
      "source 9 has 19663 features\n"
     ]
    }
   ],
   "source": [
    "for source in range(1, 10):\n",
    "    ft_train_hub_features = [f for f in train_hub_features if f.source==source]\n",
    "    print(f'source {source} has {len(ft_train_hub_features)} features')\n",
    "    with open(f'../data/pkl/train_hub_features{source}.pkl', 'wb') as f:\n",
    "        pickle.dump(ft_train_hub_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 1 has 3360 features\n",
      "source 2 has 1747 features\n",
      "source 3 has 2695 features\n",
      "source 4 has 2713 features\n",
      "source 5 has 2371 features\n",
      "source 6 has 2090 features\n",
      "source 7 has 2403 features\n",
      "source 8 has 2245 features\n",
      "source 9 has 2137 features\n"
     ]
    }
   ],
   "source": [
    "for source in range(1, 10):\n",
    "    ft_valid_hub_features = [f for f in valid_hub_features if f.source==source]\n",
    "    print(f'source {source} has {len(ft_valid_hub_features)} features')\n",
    "    with open(f'../data/pkl/valid_hub_features{source}.pkl', 'wb') as f:\n",
    "        pickle.dump(ft_valid_hub_features, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ece5f9b3f3aceda458e8d77feac6fd1c43e8580863a594c877bbe067b1219d3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('ir': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
