{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3ba5130e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Library\n",
    "import os, sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader, random_split\n",
    "\n",
    "from transformers import AutoModel, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeature():\n",
    "    \"\"\"A single set of features of data\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 input_ids,\n",
    "                 token_type_ids,\n",
    "                 attention_mask,\n",
    "                 start_positions,\n",
    "                 end_positions,\n",
    "                 offset_mapping,\n",
    "                 source=None\n",
    "                 ):\n",
    "        self.id = id\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_positions = start_positions\n",
    "        self.end_positions = end_positions\n",
    "        self.offset_mapping = offset_mapping\n",
    "        self.source = source\n",
    "\n",
    "\n",
    "def read_file(file_path, has_src=False):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    qa_data = defaultdict(list)\n",
    "    for doc in data['data']:\n",
    "        if has_src:\n",
    "            source = doc['source']\n",
    "        else:\n",
    "            source = \"all\"\n",
    "        for paragraph in doc['paragraphs']:\n",
    "            context = paragraph['context'].replace('\\u200b', '')\n",
    "            for question_and_answers in paragraph['qas']:\n",
    "                is_impossible = question_and_answers['is_impossible'] if 'is_impossible' in question_and_answers else None\n",
    "                if not is_impossible:\n",
    "                    question = question_and_answers['question']\n",
    "                    answers = question_and_answers['answers']\n",
    "                    for answer in answers:\n",
    "                        id = question + context\n",
    "                        id = hashlib.shake_256(id.encode()).hexdigest(5)\n",
    "                        qa_data['id'].append(id)\n",
    "                        qa_data['context'].append(context)\n",
    "                        qa_data['question'].append(question)\n",
    "                        qa_data['answers'].append(answer)\n",
    "                        if has_src:\n",
    "                            qa_data['source'].append(source)\n",
    "    return qa_data\n",
    "\n",
    "\n",
    "def convert_to_features(qa_data, tokenizer, max_len, has_src=False):\n",
    "    encodings = []\n",
    "    for idx, (id, context, question, answers) in tqdm(enumerate(zip(qa_data['id'],\n",
    "                                                                    qa_data['context'],\n",
    "                                                                    qa_data['question'],\n",
    "                                                                    qa_data['answers'])), total=len(qa_data['context'])):\n",
    "        encoding = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        encoding['id'] = id\n",
    "        if has_src:\n",
    "            encoding['source'] = qa_data['source'][idx]\n",
    "        else:\n",
    "            encoding['source'] = None\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "        encoding['offset_mapping'] = offset_mapping\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        sequence_ids = encoding.sequence_ids(0)\n",
    "\n",
    "        start_char = answers['answer_start']\n",
    "        end_char = start_char + len(answers['text'])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if offset_mapping[token_start_index][0] <= start_char and offset_mapping[token_end_index][1] >= end_char:\n",
    "            while token_start_index < len(offset_mapping) and offset_mapping[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            encoding[\"start_positions\"] = token_start_index - 1\n",
    "            while offset_mapping[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            encoding[\"end_positions\"] = token_end_index + 1\n",
    "            if encoding['start_positions'] < encoding['end_positions']:\n",
    "                encodings.append(encoding)\n",
    "\n",
    "    return [InputFeature(enc['id'],\n",
    "                         enc['input_ids'],\n",
    "                         enc['token_type_ids'],\n",
    "                         enc['attention_mask'],\n",
    "                         enc['start_positions'],\n",
    "                         enc['end_positions'],\n",
    "                         enc['offset_mapping'],\n",
    "                         enc['source']) for enc in encodings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ubuntu/workspace/kaist.ir/qa/data'\n",
    "squad_train_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_train.json'))\n",
    "squad_valid_data = read_file(os.path.join(data_path, 'korquad/KorQuAD_v1.0_dev.json'))\n",
    "train_hub_data = read_file(os.path.join(data_path, 'newsqa/news_train_all_10.json'), has_src=True)\n",
    "valid_hub_data = read_file(os.path.join(data_path, 'newsqa/news_test_all_10.json'), has_src=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korquad train: 60407\n",
      "korquad test: 5774\n",
      "news_hub train: 220256\n",
      "news_hub test: 23169\n"
     ]
    }
   ],
   "source": [
    "print('korquad train:', len(squad_train_data['context']))\n",
    "print('korquad test:', len(squad_valid_data['context']))\n",
    "print('news_hub train:', len(train_hub_data['context']))\n",
    "print('news_hub test:', len(valid_hub_data['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60407/60407 [01:03<00:00, 953.25it/s]\n"
     ]
    }
   ],
   "source": [
    "train_squad_features = convert_to_features(squad_train_data, tokenizer, max_len=512)\n",
    "valid_squad_features = convert_to_features(squad_valid_data, tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220256/220256 [06:54<00:00, 531.96it/s]\n",
      "100%|██████████| 23169/23169 [00:56<00:00, 409.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_hub_features = convert_to_features(train_hub_data, tokenizer, max_len=512, has_src=True)\n",
    "valid_hub_features = convert_to_features(valid_hub_data, tokenizer, max_len=512, has_src=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korquad train features: 60141\n",
      "korquad test features: 5717\n",
      "news_hub train features: 206859\n",
      "news_hub test features: 21761\n"
     ]
    }
   ],
   "source": [
    "print('korquad train features:', len(train_squad_features))\n",
    "print('korquad test features:', len(valid_squad_features))\n",
    "print('news_hub train features:', len(train_hub_features))\n",
    "print('news_hub test features:', len(valid_hub_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 32762, 4: 25466, 3: 25444, 7: 22456, 5: 22356, 8: 21861, 6: 19996, 9: 19663, 2: 16855})\n",
      "Counter({1: 3360, 4: 2713, 3: 2695, 7: 2403, 5: 2371, 8: 2245, 9: 2137, 6: 2090, 2: 1747})\n"
     ]
    }
   ],
   "source": [
    "train_sources = [f.source for f in train_hub_features]\n",
    "valid_sources = [f.source for f in valid_hub_features]\n",
    "print(Counter(train_sources))\n",
    "print(Counter(valid_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/pkl/train_squad_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_squad_features, f)\n",
    "\n",
    "# with open('../data/pkl/valid_squad_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(valid_squad_features, f)\n",
    "\n",
    "# with open('../data/pkl/train_hub_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_hub_features, f)\n",
    "\n",
    "# with open('../data/pkl/valid_hub_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(valid_hub_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pkl/train_squad_features.pkl', 'rb') as f:\n",
    "    train_squad_features = pickle.load(f)\n",
    "\n",
    "with open('../data/pkl/valid_squad_features.pkl', 'rb') as f:\n",
    "    valid_squad_features = pickle.load(f)\n",
    "\n",
    "# with open('../data/pkl/train_hub_features.pkl', 'rb') as f:\n",
    "#     train_hub_features = pickle.load(f)\n",
    "\n",
    "# with open('../data/pkl/valid_hub_features.pkl', 'rb') as f:\n",
    "#     valid_hub_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "ft_train_hub_features = random.sample(train_hub_features, k=len(train_squad_features))\n",
    "ft_valid_hub_features = random.sample(valid_hub_features, k=len(valid_squad_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/pkl/ft_train_hub_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(ft_train_hub_features, f)\n",
    "\n",
    "# with open('../data/pkl/ft_valid_hub_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(ft_valid_hub_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/pkl/ft_train_hub_features.pkl', 'rb') as f:\n",
    "#     ft_train_hub_features = pickle.load(f)\n",
    "\n",
    "# with open('../data/pkl/ft_valid_hub_features.pkl', 'rb') as f:\n",
    "#     ft_valid_hub_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counter({1: 9471, 3: 7412, 4: 7350, 5: 6571, 7: 6497, 8: 6278, 6: 5898, 9: 5804, 2: 4860})\n",
    "- Counter({1: 908, 4: 722, 3: 690, 5: 618, 9: 611, 7: 608, 8: 578, 6: 538, 2: 444})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 9\n",
    "with open(f'../data/pkl/train_hub_features{source}.pkl', 'rb') as f:\n",
    "    ft_train_hub_features = pickle.load(f)\n",
    "\n",
    "with open(f'../data/pkl/valid_hub_features{source}.pkl', 'rb') as f:\n",
    "    ft_valid_hub_features = pickle.load(f)\n",
    "\n",
    "# ft_valid_hub_features = [f for f in valid_hub_features if f.source == source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19663\n",
      "2137\n"
     ]
    }
   ],
   "source": [
    "print(len(ft_train_hub_features))\n",
    "print(len(ft_valid_hub_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 9471, 3: 7412, 4: 7350, 5: 6571, 7: 6497, 8: 6278, 6: 5898, 9: 5804, 2: 4860})\n",
      "Counter({1: 908, 4: 722, 3: 690, 5: 618, 9: 611, 7: 608, 8: 578, 6: 538, 2: 444})\n"
     ]
    }
   ],
   "source": [
    "train_sources = [f.source for f in ft_train_hub_features]\n",
    "valid_sources = [f.source for f in ft_valid_hub_features]\n",
    "print(Counter(train_sources))\n",
    "print(Counter(valid_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pkl/ft_train_book_features.pkl', 'rb') as f:\n",
    "    ft_train_book_features = pickle.load(f)\n",
    "\n",
    "with open('../data/pkl/ft_valid_book_features.pkl', 'rb') as f:\n",
    "    ft_valid_book_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_squad_features + ft_train_hub_features\n",
    "# train_features = train_squad_features + ft_train_book_features\n",
    "# Make torch\n",
    "torch_input_ids = [torch.tensor(f.input_ids, dtype=torch.long) for f in train_features]\n",
    "all_input_ids = torch.cat([ii.unsqueeze(0) for ii in torch_input_ids], dim=0)\n",
    "torch_token_type_ids = [torch.tensor(f.token_type_ids, dtype=torch.long) for f in train_features]\n",
    "all_token_type_ids = torch.cat([tti.unsqueeze(0) for tti in torch_token_type_ids], dim=0)\n",
    "torch_attention_mask = [torch.tensor(f.attention_mask, dtype=torch.long) for f in train_features]\n",
    "all_attention_mask = torch.cat([am.unsqueeze(0) for am in torch_attention_mask], dim=0)\n",
    "torch_start_positions = [torch.tensor(f.start_positions, dtype=torch.long) for f in train_features]\n",
    "all_start_positions = torch.cat([sp.unsqueeze(0) for sp in torch_start_positions], dim=0)\n",
    "torch_end_positions = [torch.tensor(f.end_positions, dtype=torch.long) for f in train_features]\n",
    "all_end_positions = torch.cat([ep.unsqueeze(0) for ep in torch_end_positions], dim=0)\n",
    "# train_dataset = TensorDataset(all_input_ids,\n",
    "#                                 all_token_type_ids,\n",
    "#                                 all_attention_mask,\n",
    "#                                 all_start_positions,\n",
    "#                                 all_end_positions)\n",
    "input_type = torch.tensor([0] * len(train_squad_features) + [1] * len(ft_train_hub_features), dtype=torch.long)\n",
    "train_dataset = TensorDataset(all_input_ids,\n",
    "                                all_token_type_ids,\n",
    "                                all_attention_mask,\n",
    "                                all_start_positions,\n",
    "                                all_end_positions,\n",
    "                                input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = valid_squad_features + ft_valid_hub_features\n",
    "# valid_features = valid_squad_features + ft_valid_book_features\n",
    "#Make Torch\n",
    "torch_input_ids = [torch.tensor(f.input_ids, dtype=torch.long) for f in valid_features]\n",
    "all_input_ids = torch.cat([ii.unsqueeze(0) for ii in torch_input_ids], dim=0)\n",
    "torch_token_type_ids = [torch.tensor(f.token_type_ids, dtype=torch.long) for f in valid_features]\n",
    "all_token_type_ids = torch.cat([tti.unsqueeze(0) for tti in torch_token_type_ids], dim=0)\n",
    "torch_attention_mask = [torch.tensor(f.attention_mask, dtype=torch.long) for f in valid_features]\n",
    "all_attention_mask = torch.cat([am.unsqueeze(0) for am in torch_attention_mask], dim=0)\n",
    "torch_start_positions = [torch.tensor(f.start_positions, dtype=torch.long) for f in valid_features]\n",
    "all_start_positions = torch.cat([sp.unsqueeze(0) for sp in torch_start_positions], dim=0)\n",
    "torch_end_positions = [torch.tensor(f.end_positions, dtype=torch.long) for f in valid_features]\n",
    "all_end_positions = torch.cat([ep.unsqueeze(0) for ep in torch_end_positions], dim=0)\n",
    "valid_dataset = TensorDataset(all_input_ids,\n",
    "                                all_token_type_ids,\n",
    "                                all_attention_mask,\n",
    "                                all_start_positions,\n",
    "                                all_end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLinearSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then linear decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1, self.warmup_steps))\n",
    "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch, lr):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss, \"lr\": lr}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) +\n",
    "          f\" examples: {loss:.3f}\" + f\" with lr: {lr}\")\n",
    "\n",
    "\n",
    "def train_batch(step, batch, model, optimizer, scheduler, beta, sigma, device, config):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "\n",
    "    input_ids = batch[0].to(device)\n",
    "    token_type_ids = batch[1].to(device)\n",
    "    attention_mask = batch[2].to(device)\n",
    "    start_positions = batch[3].to(device)\n",
    "    end_positions = batch[4].to(device)\n",
    "    if config.method == 'contrastive':\n",
    "        input_type = batch[5].to(device)\n",
    "        # Forward pass ➡\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions,\n",
    "                        input_type=input_type,\n",
    "                        beta=beta,\n",
    "                        sigma=sigma)\n",
    "    else:\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "\n",
    "    if config.gradient_accumulation_steps > 1:\n",
    "        loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        # Step with optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "    return loss, lr\n",
    "\n",
    "\n",
    "def validation(valid_loader, model, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses, acc = [], []\n",
    "        valid_example_ct = 0\n",
    "        for _, batch in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            valid_example_ct += batch[0].shape[0]\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[2].to(device)\n",
    "            start_positions = batch[3].to(device)\n",
    "            end_positions = batch[4].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "\n",
    "            loss = outputs[0].item()\n",
    "            val_losses.append(loss)\n",
    "            start_pred = torch.argmax(outputs[1], dim=1)\n",
    "            end_pred = torch.argmax(outputs[2], dim=1)\n",
    "            acc.append(\n",
    "                ((start_pred == start_positions).sum()/len(start_pred)).item())\n",
    "            acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n",
    "        avg_loss = sum(val_losses)/len(val_losses)\n",
    "        avg_acc = sum(acc)/len(acc)\n",
    "\n",
    "        print(f\"Accuracy of the model on the {valid_example_ct} \" +\n",
    "              f\"test samples: {100 * avg_acc}% with valid loss: {avg_loss}\")\n",
    "\n",
    "        wandb.log({\"valid_loss\": avg_loss, \"accuracy\": avg_acc})\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def save_model(model, tokenizer, model_path, metric):\n",
    "    print(f'saving model to {model_path}')\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, metric))\n",
    "    # model.config.to_json_file(os.path.join(model_path, CONFIG_NAME))\n",
    "    # tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config, train_dataset, valid_dataset, model):\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=config.batch_size,\n",
    "                              shuffle=True,)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                            batch_size=config.batch_size,\n",
    "                            shuffle=True,)\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    t_total = len(\n",
    "        train_loader) // config.gradient_accumulation_steps * config.epochs\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=config.learning_rate, eps=config.adam_epsilon)\n",
    "    scheduler = WarmupLinearSchedule(\n",
    "        optimizer, warmup_steps=t_total*config.warmup_proportion, t_total=t_total)\n",
    "\n",
    "    return train_loader, valid_loader, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(config, train_dataset, valid_dataset, tokenizer, model, device):\n",
    "\n",
    "    # config = wandb.config\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_loader, valid_loader, criterion, optimizer, scheduler= make(config, train_dataset, valid_dataset, model)\n",
    "    # wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    best_acc, best_loss = 0, 100.0\n",
    "    \n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            loss, lr = train_batch(step, batch, model, optimizer, scheduler, config.beta, config.sigma, device, config)\n",
    "            example_ct +=  batch[0].shape[0]\n",
    "            batch_ct += 1\n",
    "\n",
    "            if ((batch_ct + 1) % config.log_interval) == 0:\n",
    "                train_log(loss, example_ct, epoch, lr)\n",
    "        # scheduler.step()\n",
    "        avg_loss, avg_acc = validation(valid_loader, model, device)\n",
    "        \n",
    "        if config.metric == 'loss' or config.metric == \"all\":\n",
    "            if best_loss > avg_loss:\n",
    "                best_loss = avg_loss\n",
    "                print(f'best loss changed to {best_loss}')\n",
    "                save_model(model, tokenizer, config.model_path, \"pytorch_model.loss.bin\")\n",
    "        if config.metric == 'accuracy' or config.metric == \"all\":\n",
    "            if best_acc < avg_acc:\n",
    "                best_acc = avg_acc\n",
    "                print(f'best accuracy changed to {best_acc}')\n",
    "                save_model(model, tokenizer, config.model_path, \"pytorch_model.acc.bin\")\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers import PreTrainedModel, BertPreTrainedModel\n",
    "from transformers import BertModel\n",
    "#from transformers.modeling_bert import BertEncoder, BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, sigma=0.0):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        inputs_embeds = inputs_embeds + sigma * torch.randn_like(inputs_embeds, device=inputs_embeds.device)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CustomizedBertModel(BertModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = CustomizedBertEmbeddings(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        sigma=0.01,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, sigma=sigma\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class CustomizedBertForQuestionAnswering(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = CustomizedBertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "    \n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        l2_distance = ((total0-total1)**2).sum(2)\n",
    "    \n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.nansum(l2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "\n",
    "        kernel_val = [torch.exp(-l2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "    \n",
    "    def mmd(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.guassian_kernel(source, target, kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX) + torch.mean(YY) - torch.mean(XY) - torch.mean(YX)\n",
    "        return loss\n",
    "\n",
    "    # @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    # @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_type=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        beta=0.01,\n",
    "        sigma=0.01,\n",
    "    ):\n",
    "\n",
    "        if input_type is not None:\n",
    "            outputs = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                sigma=sigma,\n",
    "            )\n",
    "\n",
    "            sequence_output = outputs[0]\n",
    "\n",
    "            logits = self.qa_outputs(sequence_output)\n",
    "            start_logits, end_logits = logits.split(1, dim=-1)\n",
    "            start_logits = start_logits.squeeze(-1)\n",
    "            end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "            outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "            if start_positions is not None and end_positions is not None:\n",
    "                # If we are on multi-GPU, split add a dimension\n",
    "                if len(start_positions.size()) > 1:\n",
    "                    start_positions = start_positions.squeeze(-1)\n",
    "                if len(end_positions.size()) > 1:\n",
    "                    end_positions = end_positions.squeeze(-1)\n",
    "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "                ignored_index = start_logits.size(1)\n",
    "                start_positions.clamp_(0, ignored_index)\n",
    "                end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "                start_loss = loss_fct(start_logits, start_positions)\n",
    "                end_loss = loss_fct(end_logits, end_positions)\n",
    "\n",
    "                # find answer, context and question position mask\n",
    "                a_mask_1 = torch.zeros(token_type_ids.shape[0], token_type_ids.shape[1]+1).to(token_type_ids.device)\n",
    "                a_mask_1[torch.arange(a_mask_1.shape[0]), start_positions] = 1\n",
    "                a_mask_1 = a_mask_1.cumsum(dim=1)[:, :-1]\n",
    "                a_mask_2 = torch.zeros(token_type_ids.shape[0], token_type_ids.shape[1]+1).to(token_type_ids.device)\n",
    "                a_mask_2[torch.arange(a_mask_2.shape[0]), end_positions+1] = 1\n",
    "                a_mask_2 = a_mask_2.cumsum(dim=1)[:, :-1]\n",
    "                a_mask = a_mask_1 * (1 - a_mask_2)\n",
    "                    \n",
    "                splits = (input_ids == 102) * torch.arange(input_ids.shape[1], 0, -1).to(input_ids.device)\n",
    "                _, splits = torch.sort(splits, -1, descending=True)\n",
    "                splits = splits[:, :2]\n",
    "                # splits = (input_ids == 102).nonzero()[:, 1].reshape(input_ids.size(0),-1)\n",
    "                c_mask = (token_type_ids == 1) * attention_mask\n",
    "                c_mask[torch.arange(c_mask.size(0)), splits[:, 0]] = 0\n",
    "                c_mask[torch.arange(c_mask.size(0)), splits[:, 1]] = 0\n",
    "                c_mask = c_mask * (1 - a_mask)\n",
    "\n",
    "                q_mask = (token_type_ids == 0) * attention_mask\n",
    "                q_mask[torch.arange(q_mask.size(0)), splits[:, 0]] = 0\n",
    "                q_mask[:, 0] = 0\n",
    "                \n",
    "                a_rep = (sequence_output * a_mask.unsqueeze(-1)).sum(1) / a_mask.sum(-1).unsqueeze(-1)\n",
    "                cq_mask = ((c_mask + q_mask) > 0) * 1.0\n",
    "                cq_rep = (sequence_output * cq_mask.unsqueeze(-1)).sum(1) / cq_mask.sum(-1).unsqueeze(-1)\n",
    "\n",
    "                can_loss = -self.mmd(cq_rep, a_rep)\n",
    "                \n",
    "                if len((input_type==0).nonzero()[:, 0]) != 0 and len((input_type==1).nonzero()[:, 0]) != 0:\n",
    "                    a_rep_source = a_rep[(input_type==0).nonzero()[:, 0]].view(-1, a_rep.size(1))\n",
    "                    a_rep_target = a_rep[(input_type==1).nonzero()[:, 0]].view(-1, a_rep.size(1))\n",
    "                    cq_rep_source = cq_rep[(input_type==0).nonzero()[:, 0]].view(-1, cq_rep.size(1))\n",
    "                    cq_rep_target = cq_rep[(input_type==1).nonzero()[:, 0]].view(-1, cq_rep.size(1))\n",
    "\n",
    "                    can_loss += self.mmd(a_rep_source, a_rep_target) + self.mmd(cq_rep_source, cq_rep_target)\n",
    "                \n",
    "                total_loss = (start_loss + end_loss) / 2 + beta * can_loss\n",
    "                if torch.isnan(total_loss).any():\n",
    "                    print(start_loss, end_loss, beta, can_loss)\n",
    "                outputs = (total_loss,) + outputs\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            sigma=0.0\n",
    "        )\n",
    "\n",
    "            sequence_output = outputs[0]\n",
    "\n",
    "            logits = self.qa_outputs(sequence_output)\n",
    "            start_logits, end_logits = logits.split(1, dim=-1)\n",
    "            start_logits = start_logits.squeeze(-1).contiguous()\n",
    "            end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "            outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "\n",
    "            total_loss = None\n",
    "            if start_positions is not None and end_positions is not None:\n",
    "                # If we are on multi-GPU, split add a dimension\n",
    "                if len(start_positions.size()) > 1:\n",
    "                    start_positions = start_positions.squeeze(-1)\n",
    "                if len(end_positions.size()) > 1:\n",
    "                    end_positions = end_positions.squeeze(-1)\n",
    "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "                ignored_index = start_logits.size(1)\n",
    "                start_positions = start_positions.clamp(0, ignored_index)\n",
    "                end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "                start_loss = loss_fct(start_logits, start_positions)\n",
    "                end_loss = loss_fct(end_logits, end_positions)\n",
    "                total_loss = (start_loss + end_loss) / 2\n",
    "        return ((total_loss,) + outputs) if total_loss is not None else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "config = dict(\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    model_path='/home/ubuntu/workspace/kaist.ir/qa/model/squad_news9.contra', #change\n",
    "    method='contrastive', #change\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_proportion=0.1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    log_interval=100,\n",
    "    beta=0.001,\n",
    "    sigma=0.001,\n",
    "    metric='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing CustomizedBertForQuestionAnswering: ['bert.embeddings.position_ids', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomizedBertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CustomizedBertForQuestionAnswering.from_pretrained('klue/bert-base')\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhannabros\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hannabros/qa_contrastive/runs/2z47er27\" target=\"_blank\">kor+news9_contra_v1.0</a></strong> to <a href=\"https://wandb.ai/hannabros/qa_contrastive\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hannabros/qa_contrastive/runs/2z47er27?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3a58c73990>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"qa_contrastive\", config=config, name='kor+news9_contra_v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DotDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00792 examples: 6.079 with lr: 1.637262763966854e-06\n",
      "Loss after 01592 examples: 4.571 with lr: 3.307939053728949e-06\n",
      "Loss after 02392 examples: 2.207 with lr: 4.978615343491045e-06\n",
      "Loss after 03192 examples: 1.987 with lr: 6.6492916332531405e-06\n",
      "Loss after 03992 examples: 1.159 with lr: 8.319967923015236e-06\n",
      "Loss after 04792 examples: 2.277 with lr: 9.990644212777332e-06\n",
      "Loss after 05592 examples: 1.511 with lr: 1.1661320502539428e-05\n",
      "Loss after 06392 examples: 0.980 with lr: 1.3331996792301523e-05\n",
      "Loss after 07192 examples: 0.888 with lr: 1.500267308206362e-05\n",
      "Loss after 07992 examples: 0.455 with lr: 1.6673349371825715e-05\n",
      "Loss after 08792 examples: 0.949 with lr: 1.834402566158781e-05\n",
      "Loss after 09592 examples: 0.782 with lr: 2.0014701951349906e-05\n",
      "Loss after 10392 examples: 0.267 with lr: 2.1685378241112e-05\n",
      "Loss after 11192 examples: 1.445 with lr: 2.3356054530874097e-05\n",
      "Loss after 11992 examples: 0.654 with lr: 2.502673082063619e-05\n",
      "Loss after 12792 examples: 1.095 with lr: 2.669740711039829e-05\n",
      "Loss after 13592 examples: 0.850 with lr: 2.8368083400160384e-05\n",
      "Loss after 14392 examples: 1.008 with lr: 3.003875968992248e-05\n",
      "Loss after 15192 examples: 1.705 with lr: 3.1709435979684576e-05\n",
      "Loss after 15992 examples: 0.978 with lr: 3.338011226944667e-05\n",
      "Loss after 16792 examples: 0.298 with lr: 3.505078855920877e-05\n",
      "Loss after 17592 examples: 1.081 with lr: 3.672146484897086e-05\n",
      "Loss after 18392 examples: 1.262 with lr: 3.839214113873296e-05\n",
      "Loss after 19192 examples: 1.109 with lr: 4.0062817428495054e-05\n",
      "Loss after 19992 examples: 0.377 with lr: 4.173349371825715e-05\n",
      "Loss after 20792 examples: 0.181 with lr: 4.3404170008019245e-05\n",
      "Loss after 21592 examples: 1.430 with lr: 4.507484629778134e-05\n",
      "Loss after 22392 examples: 1.852 with lr: 4.674552258754344e-05\n",
      "Loss after 23192 examples: 0.842 with lr: 4.841619887730553e-05\n",
      "Loss after 23992 examples: 1.963 with lr: 4.9990347203659155e-05\n",
      "Loss after 24792 examples: 1.587 with lr: 4.98047165047967e-05\n",
      "Loss after 25592 examples: 0.659 with lr: 4.9619085805934244e-05\n",
      "Loss after 26392 examples: 0.700 with lr: 4.9433455107071785e-05\n",
      "Loss after 27192 examples: 0.926 with lr: 4.924782440820933e-05\n",
      "Loss after 27992 examples: 0.553 with lr: 4.9062193709346874e-05\n",
      "Loss after 28792 examples: 0.226 with lr: 4.887656301048442e-05\n",
      "Loss after 29592 examples: 1.374 with lr: 4.869093231162197e-05\n",
      "Loss after 30392 examples: 1.046 with lr: 4.850530161275951e-05\n",
      "Loss after 31192 examples: 0.338 with lr: 4.831967091389706e-05\n",
      "Loss after 31992 examples: 1.046 with lr: 4.81340402150346e-05\n",
      "Loss after 32792 examples: 1.051 with lr: 4.794840951617215e-05\n",
      "Loss after 33592 examples: 1.584 with lr: 4.7762778817309696e-05\n",
      "Loss after 34392 examples: 1.382 with lr: 4.757714811844724e-05\n",
      "Loss after 35192 examples: 1.658 with lr: 4.739151741958478e-05\n",
      "Loss after 35992 examples: 1.892 with lr: 4.720588672072233e-05\n",
      "Loss after 36792 examples: 0.724 with lr: 4.702025602185987e-05\n",
      "Loss after 37592 examples: 0.375 with lr: 4.6834625322997416e-05\n",
      "Loss after 38392 examples: 1.028 with lr: 4.6648994624134964e-05\n",
      "Loss after 39192 examples: 0.959 with lr: 4.6463363925272505e-05\n",
      "Loss after 39992 examples: 0.259 with lr: 4.627773322641005e-05\n",
      "Loss after 40792 examples: 0.425 with lr: 4.60921025275476e-05\n",
      "Loss after 41592 examples: 0.959 with lr: 4.590647182868514e-05\n",
      "Loss after 42392 examples: 0.192 with lr: 4.572084112982269e-05\n",
      "Loss after 43192 examples: 0.495 with lr: 4.553521043096023e-05\n",
      "Loss after 43992 examples: 0.177 with lr: 4.534957973209777e-05\n",
      "Loss after 44792 examples: 0.203 with lr: 4.516394903323532e-05\n",
      "Loss after 45592 examples: 0.043 with lr: 4.497831833437287e-05\n",
      "Loss after 46392 examples: 0.259 with lr: 4.479268763551041e-05\n",
      "Loss after 47192 examples: 0.615 with lr: 4.460705693664796e-05\n",
      "Loss after 47992 examples: 0.522 with lr: 4.44214262377855e-05\n",
      "Loss after 48792 examples: 0.782 with lr: 4.4235795538923046e-05\n",
      "Loss after 49592 examples: 0.638 with lr: 4.4050164840060594e-05\n",
      "Loss after 50392 examples: 1.425 with lr: 4.3864534141198135e-05\n",
      "Loss after 51192 examples: 0.699 with lr: 4.3678903442335677e-05\n",
      "Loss after 51992 examples: 0.316 with lr: 4.3493272743473224e-05\n",
      "Loss after 52792 examples: 0.466 with lr: 4.3307642044610766e-05\n",
      "Loss after 53592 examples: 0.141 with lr: 4.3122011345748314e-05\n",
      "Loss after 54392 examples: 0.042 with lr: 4.293638064688586e-05\n",
      "Loss after 55192 examples: 1.019 with lr: 4.27507499480234e-05\n",
      "Loss after 55992 examples: 0.354 with lr: 4.256511924916095e-05\n",
      "Loss after 56792 examples: 0.468 with lr: 4.23794885502985e-05\n",
      "Loss after 57592 examples: 0.921 with lr: 4.219385785143604e-05\n",
      "Loss after 58392 examples: 0.734 with lr: 4.200822715257359e-05\n",
      "Loss after 59192 examples: 1.960 with lr: 4.1822596453711136e-05\n",
      "Loss after 59992 examples: 0.042 with lr: 4.163696575484867e-05\n",
      "Loss after 60792 examples: 0.049 with lr: 4.145133505598622e-05\n",
      "Loss after 61592 examples: 0.141 with lr: 4.1265704357123766e-05\n",
      "Loss after 62392 examples: 1.438 with lr: 4.108007365826131e-05\n",
      "Loss after 63192 examples: 0.320 with lr: 4.0894442959398855e-05\n",
      "Loss after 63992 examples: 0.410 with lr: 4.07088122605364e-05\n",
      "Loss after 64792 examples: 0.360 with lr: 4.0523181561673944e-05\n",
      "Loss after 65592 examples: 0.491 with lr: 4.033755086281149e-05\n",
      "Loss after 66392 examples: 0.829 with lr: 4.015192016394903e-05\n",
      "Loss after 67192 examples: 0.285 with lr: 3.996628946508658e-05\n",
      "Loss after 67992 examples: 0.506 with lr: 3.978065876622412e-05\n",
      "Loss after 68792 examples: 2.087 with lr: 3.959502806736167e-05\n",
      "Loss after 69592 examples: 0.353 with lr: 3.940939736849921e-05\n",
      "Loss after 70392 examples: 0.466 with lr: 3.922376666963676e-05\n",
      "Loss after 71192 examples: 1.542 with lr: 3.90381359707743e-05\n",
      "Loss after 71992 examples: 1.322 with lr: 3.885250527191185e-05\n",
      "Loss after 72792 examples: 0.732 with lr: 3.8666874573049396e-05\n",
      "Loss after 73592 examples: 0.559 with lr: 3.848124387418694e-05\n",
      "Loss after 74392 examples: 0.498 with lr: 3.8295613175324485e-05\n",
      "Loss after 75192 examples: 0.281 with lr: 3.810998247646203e-05\n",
      "Loss after 75992 examples: 0.407 with lr: 3.792435177759957e-05\n",
      "Loss after 76792 examples: 0.670 with lr: 3.7738721078737116e-05\n",
      "Loss after 77592 examples: 1.181 with lr: 3.7553090379874664e-05\n",
      "Loss after 78392 examples: 0.194 with lr: 3.7367459681012205e-05\n",
      "Loss after 79192 examples: 0.098 with lr: 3.718182898214975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [01:21<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 7854 test samples: 78.22895451864255% with valid loss: 0.8009827233634722\n",
      "best loss changed to 0.8009827233634722\n",
      "saving model to /home/ubuntu/workspace/kaist.ir/qa/model/squad_news9.contra\n",
      "best accuracy changed to 0.7822895451864255\n",
      "saving model to /home/ubuntu/workspace/kaist.ir/qa/model/squad_news9.contra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [48:43<1:37:27, 2923.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 79988 examples: 0.663 with lr: 3.69961982832873e-05\n",
      "Loss after 80788 examples: 0.437 with lr: 3.681056758442484e-05\n",
      "Loss after 81588 examples: 0.054 with lr: 3.662493688556239e-05\n",
      "Loss after 82388 examples: 0.834 with lr: 3.643930618669993e-05\n",
      "Loss after 83188 examples: 0.459 with lr: 3.625367548783748e-05\n",
      "Loss after 83988 examples: 0.006 with lr: 3.606804478897503e-05\n",
      "Loss after 84788 examples: 0.655 with lr: 3.588241409011257e-05\n",
      "Loss after 85588 examples: 0.932 with lr: 3.569678339125011e-05\n",
      "Loss after 86388 examples: 0.366 with lr: 3.551115269238766e-05\n",
      "Loss after 87188 examples: 0.661 with lr: 3.53255219935252e-05\n",
      "Loss after 87988 examples: 0.163 with lr: 3.5139891294662746e-05\n",
      "Loss after 88788 examples: 0.206 with lr: 3.4954260595800294e-05\n",
      "Loss after 89588 examples: 0.210 with lr: 3.4768629896937835e-05\n",
      "Loss after 90388 examples: 1.317 with lr: 3.458299919807538e-05\n",
      "Loss after 91188 examples: 0.811 with lr: 3.439736849921293e-05\n",
      "Loss after 91988 examples: 0.038 with lr: 3.421173780035047e-05\n",
      "Loss after 92788 examples: 0.122 with lr: 3.4026107101488013e-05\n",
      "Loss after 93588 examples: 0.588 with lr: 3.384047640262556e-05\n",
      "Loss after 94388 examples: 0.361 with lr: 3.36548457037631e-05\n",
      "Loss after 95188 examples: 0.017 with lr: 3.346921500490065e-05\n",
      "Loss after 95988 examples: 1.172 with lr: 3.32835843060382e-05\n",
      "Loss after 96788 examples: 0.104 with lr: 3.309795360717574e-05\n",
      "Loss after 97588 examples: 0.082 with lr: 3.291232290831329e-05\n",
      "Loss after 98388 examples: 0.920 with lr: 3.2726692209450835e-05\n",
      "Loss after 99188 examples: 0.781 with lr: 3.2541061510588377e-05\n",
      "Loss after 99988 examples: 0.185 with lr: 3.2355430811725925e-05\n",
      "Loss after 100788 examples: 1.196 with lr: 3.2169800112863466e-05\n",
      "Loss after 101588 examples: 1.436 with lr: 3.198416941400101e-05\n",
      "Loss after 102388 examples: 0.063 with lr: 3.1798538715138555e-05\n",
      "Loss after 103188 examples: 0.476 with lr: 3.16129080162761e-05\n",
      "Loss after 103988 examples: 0.393 with lr: 3.1427277317413644e-05\n",
      "Loss after 104788 examples: 0.380 with lr: 3.124164661855119e-05\n",
      "Loss after 105588 examples: 0.719 with lr: 3.105601591968873e-05\n",
      "Loss after 106388 examples: 1.143 with lr: 3.087038522082628e-05\n",
      "Loss after 107188 examples: 0.435 with lr: 3.068475452196383e-05\n",
      "Loss after 107988 examples: 0.364 with lr: 3.0499123823101373e-05\n",
      "Loss after 108788 examples: 1.608 with lr: 3.0313493124238918e-05\n",
      "Loss after 109588 examples: 0.023 with lr: 3.012786242537646e-05\n",
      "Loss after 110388 examples: 0.498 with lr: 2.9942231726514004e-05\n",
      "Loss after 111188 examples: 0.303 with lr: 2.9756601027651548e-05\n",
      "Loss after 111988 examples: 1.328 with lr: 2.9570970328789093e-05\n",
      "Loss after 112788 examples: 0.092 with lr: 2.938533962992664e-05\n",
      "Loss after 113588 examples: 1.119 with lr: 2.9199708931064185e-05\n",
      "Loss after 114388 examples: 0.857 with lr: 2.901407823220173e-05\n",
      "Loss after 115188 examples: 2.475 with lr: 2.8828447533339274e-05\n",
      "Loss after 115988 examples: 0.139 with lr: 2.8642816834476822e-05\n",
      "Loss after 116788 examples: 0.978 with lr: 2.8457186135614367e-05\n",
      "Loss after 117588 examples: 0.691 with lr: 2.8271555436751908e-05\n",
      "Loss after 118388 examples: 0.022 with lr: 2.8085924737889453e-05\n",
      "Loss after 119188 examples: 0.308 with lr: 2.7900294039026997e-05\n",
      "Loss after 119988 examples: 0.525 with lr: 2.771466334016454e-05\n",
      "Loss after 120788 examples: 0.176 with lr: 2.752903264130209e-05\n",
      "Loss after 121588 examples: 0.094 with lr: 2.7343401942439634e-05\n",
      "Loss after 122388 examples: 0.119 with lr: 2.715777124357718e-05\n",
      "Loss after 123188 examples: 0.538 with lr: 2.6972140544714723e-05\n",
      "Loss after 123988 examples: 1.123 with lr: 2.678650984585227e-05\n",
      "Loss after 124788 examples: 0.803 with lr: 2.6600879146989816e-05\n",
      "Loss after 125588 examples: 0.411 with lr: 2.6415248448127357e-05\n",
      "Loss after 126388 examples: 0.380 with lr: 2.62296177492649e-05\n",
      "Loss after 127188 examples: 0.340 with lr: 2.6043987050402446e-05\n",
      "Loss after 127988 examples: 0.107 with lr: 2.585835635153999e-05\n",
      "Loss after 128788 examples: 0.207 with lr: 2.567272565267754e-05\n",
      "Loss after 129588 examples: 0.015 with lr: 2.5487094953815083e-05\n",
      "Loss after 130388 examples: 0.531 with lr: 2.5301464254952628e-05\n",
      "Loss after 131188 examples: 0.593 with lr: 2.5115833556090172e-05\n",
      "Loss after 131988 examples: 0.852 with lr: 2.4930202857227717e-05\n",
      "Loss after 132788 examples: 0.394 with lr: 2.474457215836526e-05\n",
      "Loss after 133588 examples: 0.247 with lr: 2.4558941459502806e-05\n",
      "Loss after 134388 examples: 0.142 with lr: 2.4373310760640354e-05\n",
      "Loss after 135188 examples: 0.846 with lr: 2.4187680061777898e-05\n",
      "Loss after 135988 examples: 0.604 with lr: 2.400204936291544e-05\n",
      "Loss after 136788 examples: 0.743 with lr: 2.3816418664052987e-05\n",
      "Loss after 137588 examples: 0.403 with lr: 2.3630787965190532e-05\n",
      "Loss after 138388 examples: 0.249 with lr: 2.3445157266328076e-05\n",
      "Loss after 139188 examples: 0.385 with lr: 2.3259526567465624e-05\n",
      "Loss after 139988 examples: 0.669 with lr: 2.3073895868603165e-05\n",
      "Loss after 140788 examples: 0.251 with lr: 2.288826516974071e-05\n",
      "Loss after 141588 examples: 0.897 with lr: 2.2702634470878255e-05\n",
      "Loss after 142388 examples: 0.362 with lr: 2.2517003772015803e-05\n",
      "Loss after 143188 examples: 0.387 with lr: 2.2331373073153347e-05\n",
      "Loss after 143988 examples: 0.129 with lr: 2.2145742374290888e-05\n",
      "Loss after 144788 examples: 0.093 with lr: 2.1960111675428436e-05\n",
      "Loss after 145588 examples: 0.115 with lr: 2.177448097656598e-05\n",
      "Loss after 146388 examples: 0.471 with lr: 2.1588850277703525e-05\n",
      "Loss after 147188 examples: 0.173 with lr: 2.1403219578841073e-05\n",
      "Loss after 147988 examples: 0.172 with lr: 2.1217588879978618e-05\n",
      "Loss after 148788 examples: 0.090 with lr: 2.103195818111616e-05\n",
      "Loss after 149588 examples: 0.134 with lr: 2.0846327482253707e-05\n",
      "Loss after 150388 examples: 0.035 with lr: 2.066069678339125e-05\n",
      "Loss after 151188 examples: 1.022 with lr: 2.0475066084528796e-05\n",
      "Loss after 151988 examples: 0.351 with lr: 2.028943538566634e-05\n",
      "Loss after 152788 examples: 0.294 with lr: 2.0103804686803885e-05\n",
      "Loss after 153588 examples: 0.217 with lr: 1.991817398794143e-05\n",
      "Loss after 154388 examples: 0.849 with lr: 1.9732543289078974e-05\n",
      "Loss after 155188 examples: 0.960 with lr: 1.9546912590216522e-05\n",
      "Loss after 155988 examples: 0.170 with lr: 1.9361281891354067e-05\n",
      "Loss after 156788 examples: 0.149 with lr: 1.9175651192491608e-05\n",
      "Loss after 157588 examples: 0.545 with lr: 1.8990020493629156e-05\n",
      "Loss after 158388 examples: 0.041 with lr: 1.88043897947667e-05\n",
      "Loss after 159188 examples: 0.018 with lr: 1.8618759095904245e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [01:21<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 7854 test samples: 80.10225730005448% with valid loss: 0.7655609007969844\n",
      "best loss changed to 0.7655609007969844\n",
      "saving model to /home/ubuntu/workspace/kaist.ir/qa/model/squad_news9.contra\n",
      "best accuracy changed to 0.8010225730005447\n",
      "saving model to /home/ubuntu/workspace/kaist.ir/qa/model/squad_news9.contra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [1:37:48<48:56, 2936.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 159984 examples: 0.275 with lr: 1.843312839704179e-05\n",
      "Loss after 160784 examples: -0.003 with lr: 1.8247497698179334e-05\n",
      "Loss after 161584 examples: 0.423 with lr: 1.806186699931688e-05\n",
      "Loss after 162384 examples: 0.175 with lr: 1.7876236300454423e-05\n",
      "Loss after 163184 examples: 0.015 with lr: 1.769060560159197e-05\n",
      "Loss after 163984 examples: 0.203 with lr: 1.7504974902729516e-05\n",
      "Loss after 164784 examples: 0.117 with lr: 1.7319344203867057e-05\n",
      "Loss after 165584 examples: 0.004 with lr: 1.7133713505004605e-05\n",
      "Loss after 166384 examples: 0.031 with lr: 1.694808280614215e-05\n",
      "Loss after 167184 examples: 0.202 with lr: 1.6762452107279694e-05\n",
      "Loss after 167984 examples: 0.703 with lr: 1.6576821408417238e-05\n",
      "Loss after 168784 examples: 0.090 with lr: 1.6391190709554783e-05\n",
      "Loss after 169584 examples: 0.104 with lr: 1.6205560010692327e-05\n",
      "Loss after 170384 examples: 0.945 with lr: 1.6019929311829872e-05\n",
      "Loss after 171184 examples: 0.024 with lr: 1.583429861296742e-05\n",
      "Loss after 171984 examples: 0.177 with lr: 1.5648667914104964e-05\n",
      "Loss after 172784 examples: 0.030 with lr: 1.546303721524251e-05\n",
      "Loss after 173584 examples: 0.200 with lr: 1.5277406516380053e-05\n",
      "Loss after 174384 examples: 0.010 with lr: 1.5091775817517598e-05\n",
      "Loss after 175184 examples: 0.517 with lr: 1.4906145118655143e-05\n",
      "Loss after 175984 examples: 1.624 with lr: 1.4720514419792689e-05\n",
      "Loss after 176784 examples: 0.093 with lr: 1.4534883720930233e-05\n",
      "Loss after 177584 examples: 0.292 with lr: 1.4349253022067776e-05\n",
      "Loss after 178384 examples: 0.129 with lr: 1.4163622323205322e-05\n",
      "Loss after 179184 examples: 0.047 with lr: 1.3977991624342867e-05\n",
      "Loss after 179984 examples: 0.588 with lr: 1.3792360925480413e-05\n",
      "Loss after 180784 examples: 0.021 with lr: 1.3606730226617958e-05\n",
      "Loss after 181584 examples: 0.095 with lr: 1.34210995277555e-05\n",
      "Loss after 182384 examples: 0.305 with lr: 1.3235468828893047e-05\n",
      "Loss after 183184 examples: 0.153 with lr: 1.3049838130030591e-05\n",
      "Loss after 183984 examples: 1.079 with lr: 1.2864207431168138e-05\n",
      "Loss after 184784 examples: 0.007 with lr: 1.2678576732305684e-05\n",
      "Loss after 185584 examples: 0.044 with lr: 1.2492946033443227e-05\n",
      "Loss after 186384 examples: 0.025 with lr: 1.2307315334580771e-05\n",
      "Loss after 187184 examples: 0.041 with lr: 1.2121684635718316e-05\n",
      "Loss after 187984 examples: 0.007 with lr: 1.1936053936855862e-05\n",
      "Loss after 188784 examples: 0.045 with lr: 1.1750423237993407e-05\n",
      "Loss after 189584 examples: 0.348 with lr: 1.1564792539130951e-05\n",
      "Loss after 190384 examples: 0.090 with lr: 1.1379161840268497e-05\n",
      "Loss after 191184 examples: 0.087 with lr: 1.1193531141406042e-05\n",
      "Loss after 191984 examples: 0.352 with lr: 1.1007900442543587e-05\n",
      "Loss after 192784 examples: 0.002 with lr: 1.0822269743681131e-05\n",
      "Loss after 193584 examples: 0.005 with lr: 1.0636639044818676e-05\n",
      "Loss after 194384 examples: 0.102 with lr: 1.0451008345956222e-05\n",
      "Loss after 195184 examples: 0.486 with lr: 1.0265377647093766e-05\n",
      "Loss after 195984 examples: 0.523 with lr: 1.0079746948231311e-05\n",
      "Loss after 196784 examples: 0.001 with lr: 9.894116249368856e-06\n",
      "Loss after 197584 examples: 0.627 with lr: 9.7084855505064e-06\n",
      "Loss after 198384 examples: 0.109 with lr: 9.522854851643946e-06\n",
      "Loss after 199184 examples: 0.059 with lr: 9.337224152781491e-06\n",
      "Loss after 199984 examples: 0.187 with lr: 9.151593453919035e-06\n",
      "Loss after 200784 examples: 0.017 with lr: 8.96596275505658e-06\n",
      "Loss after 201584 examples: 0.555 with lr: 8.780332056194125e-06\n",
      "Loss after 202384 examples: 0.147 with lr: 8.59470135733167e-06\n",
      "Loss after 203184 examples: 0.176 with lr: 8.409070658469215e-06\n",
      "Loss after 203984 examples: 0.004 with lr: 8.22343995960676e-06\n",
      "Loss after 204784 examples: 0.032 with lr: 8.037809260744306e-06\n",
      "Loss after 205584 examples: 0.392 with lr: 7.852178561881849e-06\n",
      "Loss after 206384 examples: 0.082 with lr: 7.666547863019395e-06\n",
      "Loss after 207184 examples: 0.171 with lr: 7.480917164156939e-06\n",
      "Loss after 207984 examples: 0.004 with lr: 7.295286465294484e-06\n",
      "Loss after 208784 examples: 0.818 with lr: 7.10965576643203e-06\n",
      "Loss after 209584 examples: 0.008 with lr: 6.924025067569574e-06\n",
      "Loss after 210384 examples: 0.581 with lr: 6.73839436870712e-06\n",
      "Loss after 211184 examples: 1.144 with lr: 6.552763669844663e-06\n",
      "Loss after 211984 examples: 0.006 with lr: 6.367132970982209e-06\n",
      "Loss after 212784 examples: 0.336 with lr: 6.181502272119754e-06\n",
      "Loss after 213584 examples: 0.429 with lr: 5.9958715732572996e-06\n",
      "Loss after 214384 examples: 0.040 with lr: 5.810240874394844e-06\n",
      "Loss after 215184 examples: 0.053 with lr: 5.624610175532389e-06\n",
      "Loss after 215984 examples: 0.281 with lr: 5.438979476669934e-06\n",
      "Loss after 216784 examples: 0.591 with lr: 5.2533487778074794e-06\n",
      "Loss after 217584 examples: 0.416 with lr: 5.067718078945024e-06\n",
      "Loss after 218384 examples: 0.360 with lr: 4.8820873800825685e-06\n",
      "Loss after 219184 examples: 0.009 with lr: 4.696456681220113e-06\n",
      "Loss after 219984 examples: 0.023 with lr: 4.5108259823576585e-06\n",
      "Loss after 220784 examples: -0.000 with lr: 4.325195283495204e-06\n",
      "Loss after 221584 examples: 0.391 with lr: 4.1395645846327484e-06\n",
      "Loss after 222384 examples: 0.696 with lr: 3.953933885770293e-06\n",
      "Loss after 223184 examples: 0.262 with lr: 3.768303186907838e-06\n",
      "Loss after 223984 examples: 0.222 with lr: 3.5826724880453833e-06\n",
      "Loss after 224784 examples: 0.003 with lr: 3.3970417891829283e-06\n",
      "Loss after 225584 examples: 0.142 with lr: 3.211411090320473e-06\n",
      "Loss after 226384 examples: 0.365 with lr: 3.025780391458018e-06\n",
      "Loss after 227184 examples: 0.039 with lr: 2.840149692595563e-06\n",
      "Loss after 227984 examples: 0.040 with lr: 2.6545189937331074e-06\n",
      "Loss after 228784 examples: 0.434 with lr: 2.4688882948706528e-06\n",
      "Loss after 229584 examples: 0.860 with lr: 2.2832575960081973e-06\n",
      "Loss after 230384 examples: 0.014 with lr: 2.0976268971457427e-06\n",
      "Loss after 231184 examples: 0.038 with lr: 1.9119961982832873e-06\n",
      "Loss after 231984 examples: 0.040 with lr: 1.7263654994208324e-06\n",
      "Loss after 232784 examples: 0.012 with lr: 1.5407348005583772e-06\n",
      "Loss after 233584 examples: 0.077 with lr: 1.3551041016959222e-06\n",
      "Loss after 234384 examples: 0.367 with lr: 1.169473402833467e-06\n",
      "Loss after 235184 examples: 0.596 with lr: 9.83842703971012e-07\n",
      "Loss after 235984 examples: 0.100 with lr: 7.982120051085569e-07\n",
      "Loss after 236784 examples: 0.050 with lr: 6.125813062461017e-07\n",
      "Loss after 237584 examples: 0.466 with lr: 4.2695060738364666e-07\n",
      "Loss after 238384 examples: 0.703 with lr: 2.4131990852119163e-07\n",
      "Loss after 239184 examples: 0.013 with lr: 5.5689209658736524e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [01:21<00:00, 12.02it/s]\n",
      "100%|██████████| 3/3 [2:26:46<00:00, 2935.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 7854 test samples: 79.7479633401222% with valid loss: 0.9962342525576289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_pipeline(config, train_dataset, valid_dataset, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13670... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbae96346e9487dbef68c63c48aff65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁█▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>loss</td><td>▇▄▄▄▃▄▄▂▂▁▁▂▃▃▂▁▄▁▆█▁▁▁▂▁▁▁▁▁▆▂▁▂▁▁▄▂▂▁▁</td></tr><tr><td>lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>valid_loss</td><td>▂▁█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.79748</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>loss</td><td>0.0129</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>valid_loss</td><td>0.99623</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">kor+news9_contra_v1.0</strong>: <a href=\"https://wandb.ai/hannabros/qa_contrastive/runs/2z47er27\" target=\"_blank\">https://wandb.ai/hannabros/qa_contrastive/runs/2z47er27</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211213_151515-2z47er27/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=8,\n",
    "                              shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CustomizedBertForQuestionAnswering.from_pretrained('klue/bert-base')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_loader) // config.gradient_accumulation_steps * config.epochs\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=config.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=t_total*config.warmup_proportion, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing CustomizedBertForQuestionAnswering: ['bert.embeddings.position_ids', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomizedBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomizedBertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/15036 [00:00<1:22:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3301, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 501/15036 [02:22<1:09:10,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7735, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1001/15036 [04:44<1:06:23,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1346, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1501/15036 [07:07<1:04:45,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8991, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2001/15036 [09:29<1:01:35,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7134, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2501/15036 [11:51<59:19,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8837, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 3001/15036 [14:13<57:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5410, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3501/15036 [16:35<55:20,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4001/15036 [18:58<52:50,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7466, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 4501/15036 [21:21<49:53,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7122, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5001/15036 [23:44<48:14,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9319, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 5501/15036 [26:07<45:11,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 6001/15036 [28:29<43:07,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3111, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6501/15036 [30:53<41:43,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2169, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7001/15036 [33:17<38:24,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3835, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 7501/15036 [35:41<35:58,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8966, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8001/15036 [38:04<33:44,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8501/15036 [40:27<30:51,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8870, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 9001/15036 [42:51<28:37,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5651, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9501/15036 [45:14<26:31,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4761, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10001/15036 [47:37<23:48,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4985, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 10501/15036 [49:59<21:40,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7667, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11001/15036 [52:22<19:19,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4789, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 11501/15036 [54:48<16:53,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1643, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 12001/15036 [57:13<14:29,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2914, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 12501/15036 [59:36<12:05,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6527, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 13001/15036 [1:01:59<09:39,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2810, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 13501/15036 [1:04:21<07:20,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5107, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14001/15036 [1:06:44<04:56,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1934, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 14501/15036 [1:09:08<02:31,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6701, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 15001/15036 [1:11:31<00:09,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8176, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15036/15036 [1:11:41<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "class CustomizedBertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, sigma=0.0):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        inputs_embeds = inputs_embeds + sigma * torch.randn_like(inputs_embeds, device=inputs_embeds.device)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CustomizedBertModel(BertModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = CustomizedBertEmbeddings(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        sigma=0.01,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, sigma=sigma\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class CustomizedBertForQuestionAnswering(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = CustomizedBertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "    \n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        l2_distance = ((total0-total1)**2).sum(2)\n",
    "    \n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.nansum(l2_distance.data) / (n_samples**2-n_samples)\n",
    "            # bandwidth = torch.sum(l2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "\n",
    "        kernel_val = [torch.exp(-l2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        for val in kernel_val:\n",
    "            if torch.isnan(val).any():\n",
    "                print(kernel_val)\n",
    "        return sum(kernel_val)\n",
    "    \n",
    "    def mmd(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.guassian_kernel(source, target, kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX) + torch.mean(YY) - torch.mean(XY) - torch.mean(YX)\n",
    "        if torch.isnan(loss).any():\n",
    "            print(loss)\n",
    "        return loss\n",
    "\n",
    "    # @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    # @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_type=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        beta=0.01,\n",
    "        sigma=0.01,\n",
    "    ):\n",
    "\n",
    "        if input_type is not None:\n",
    "            outputs = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                sigma=sigma,\n",
    "            )\n",
    "\n",
    "            sequence_output = outputs[0]\n",
    "\n",
    "            logits = self.qa_outputs(sequence_output)\n",
    "            start_logits, end_logits = logits.split(1, dim=-1)\n",
    "            start_logits = start_logits.squeeze(-1)\n",
    "            end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "            outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "            if start_positions is not None and end_positions is not None:\n",
    "                # If we are on multi-GPU, split add a dimension\n",
    "                if len(start_positions.size()) > 1:\n",
    "                    start_positions = start_positions.squeeze(-1)\n",
    "                if len(end_positions.size()) > 1:\n",
    "                    end_positions = end_positions.squeeze(-1)\n",
    "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "                ignored_index = start_logits.size(1)\n",
    "                start_positions.clamp_(0, ignored_index)\n",
    "                end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "                start_loss = loss_fct(start_logits, start_positions)\n",
    "                end_loss = loss_fct(end_logits, end_positions)\n",
    "\n",
    "                # find answer, context and question position mask\n",
    "                a_mask_1 = torch.zeros(token_type_ids.shape[0], token_type_ids.shape[1]+1).to(token_type_ids.device)\n",
    "                a_mask_1[torch.arange(a_mask_1.shape[0]), start_positions] = 1\n",
    "                a_mask_1 = a_mask_1.cumsum(dim=1)[:, :-1]\n",
    "                a_mask_2 = torch.zeros(token_type_ids.shape[0], token_type_ids.shape[1]+1).to(token_type_ids.device)\n",
    "                a_mask_2[torch.arange(a_mask_2.shape[0]), end_positions+1] = 1\n",
    "                a_mask_2 = a_mask_2.cumsum(dim=1)[:, :-1]\n",
    "                a_mask = a_mask_1 * (1 - a_mask_2)\n",
    "                    \n",
    "                splits = (input_ids == 102) * torch.arange(input_ids.shape[1], 0, -1).to(input_ids.device)\n",
    "                _, splits = torch.sort(splits, -1, descending=True)\n",
    "                splits = splits[:, :2]\n",
    "                # splits = (input_ids == 102).nonzero()[:, 1].reshape(input_ids.size(0),-1)\n",
    "                c_mask = (token_type_ids == 1) * attention_mask\n",
    "                c_mask[torch.arange(c_mask.size(0)), splits[:, 0]] = 0\n",
    "                c_mask[torch.arange(c_mask.size(0)), splits[:, 1]] = 0\n",
    "                c_mask = c_mask * (1 - a_mask)\n",
    "\n",
    "                q_mask = (token_type_ids == 0) * attention_mask\n",
    "                q_mask[torch.arange(q_mask.size(0)), splits[:, 0]] = 0\n",
    "                q_mask[:, 0] = 0\n",
    "                \n",
    "                a_rep = (sequence_output * a_mask.unsqueeze(-1)).sum(1) / a_mask.sum(-1).unsqueeze(-1)\n",
    "                cq_mask = ((c_mask + q_mask) > 0) * 1.0\n",
    "                cq_rep = (sequence_output * cq_mask.unsqueeze(-1)).sum(1) / cq_mask.sum(-1).unsqueeze(-1)\n",
    "                \n",
    "                if torch.isnan(cq_rep).any() or torch.isnan(a_rep).any():\n",
    "                    print(cq_rep)\n",
    "                    print(a_rep)\n",
    "                can_loss = -self.mmd(cq_rep, a_rep)\n",
    "                if torch.isnan(can_loss).any():\n",
    "                    print(can_loss)\n",
    "                \n",
    "                if len((input_type==0).nonzero()[:, 0]) != 0 and len((input_type==1).nonzero()[:, 0]) != 0:\n",
    "                    a_rep_source = a_rep[(input_type==0).nonzero()[:, 0]].view(-1, a_rep.size(1))\n",
    "                    a_rep_target = a_rep[(input_type==1).nonzero()[:, 0]].view(-1, a_rep.size(1))\n",
    "                    cq_rep_source = cq_rep[(input_type==0).nonzero()[:, 0]].view(-1, cq_rep.size(1))\n",
    "                    cq_rep_target = cq_rep[(input_type==1).nonzero()[:, 0]].view(-1, cq_rep.size(1))\n",
    "\n",
    "                    if torch.isnan(a_rep_source).any() or torch.isnan(a_rep_target).any() or torch.isnan(cq_rep_source).any() or torch.isnan(cq_rep_target).any():\n",
    "                        print(a_rep_source)\n",
    "                        print(a_rep_target)\n",
    "                        print(cq_rep_source)\n",
    "                        print(cq_rep_target)\n",
    "                    can_loss += self.mmd(a_rep_source, a_rep_target) + self.mmd(cq_rep_source, cq_rep_target)\n",
    "                    if torch.isnan(can_loss).any():\n",
    "                        print(can_loss)\n",
    "                \n",
    "                total_loss = (start_loss + end_loss) / 2 + beta * can_loss\n",
    "                if torch.isnan(total_loss).any():\n",
    "                    print(start_loss, end_loss, beta, can_loss)\n",
    "                outputs = (total_loss,) + outputs\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            sigma=0.0\n",
    "        )\n",
    "\n",
    "            sequence_output = outputs[0]\n",
    "\n",
    "            logits = self.qa_outputs(sequence_output)\n",
    "            start_logits, end_logits = logits.split(1, dim=-1)\n",
    "            start_logits = start_logits.squeeze(-1).contiguous()\n",
    "            end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "            outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "\n",
    "            total_loss = None\n",
    "            if start_positions is not None and end_positions is not None:\n",
    "                # If we are on multi-GPU, split add a dimension\n",
    "                if len(start_positions.size()) > 1:\n",
    "                    start_positions = start_positions.squeeze(-1)\n",
    "                if len(end_positions.size()) > 1:\n",
    "                    end_positions = end_positions.squeeze(-1)\n",
    "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "                ignored_index = start_logits.size(1)\n",
    "                start_positions = start_positions.clamp(0, ignored_index)\n",
    "                end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "                start_loss = loss_fct(start_logits, start_positions)\n",
    "                end_loss = loss_fct(end_logits, end_positions)\n",
    "                total_loss = (start_loss + end_loss) / 2\n",
    "        return ((total_loss,) + outputs) if total_loss is not None else outputs\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CustomizedBertForQuestionAnswering.from_pretrained('klue/bert-base')\n",
    "model = model.to(device)\n",
    "\n",
    "t_total = len(train_loader) // config.gradient_accumulation_steps * config.epochs\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=config.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=t_total*config.warmup_proportion, t_total=t_total)\n",
    "\n",
    "for idx, d in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    input_ids = d[0].to(device)\n",
    "    token_type_ids = d[1].to(device)\n",
    "    attention_mask = d[2].to(device)\n",
    "    start_positions = d[3].to(device)\n",
    "    end_positions = d[4].to(device)\n",
    "    input_type = d[5].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions,\n",
    "                    input_type=input_type,\n",
    "                    beta=0.01,\n",
    "                    sigma=0.01)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    if idx % 500 == 0:\n",
    "        print(loss)\n",
    "    if torch.isnan(loss).any():\n",
    "        print(input_ids, token_type_ids, attention_mask, start_positions, end_positions, input_type)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>) 0.01 tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_outputs = model(input_ids=input_ids,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions,\n",
    "                    input_type=input_type,\n",
    "                    beta=0.01,\n",
    "                    sigma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ece5f9b3f3aceda458e8d77feac6fd1c43e8580863a594c877bbe067b1219d3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('ir': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
